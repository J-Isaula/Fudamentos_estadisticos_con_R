[
  {
    "objectID": "summary.html#qué-son-las-probabilidades",
    "href": "summary.html#qué-son-las-probabilidades",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.1 ¿Qué son las probabilidades?",
    "text": "2.1 ¿Qué son las probabilidades?\nLa gente habla de suerte con bastante frecuencia, como ¿cuales son las posibilidades de cerrar una venta, de que llueva mañana o de ganar un juego? Pero, ¿Cómo medimos exactamente el azar?\nPodemos medir las probabilidades o posibilidades de un evento usando la probabilidad. Podemos calcular la probabilidad de ocurrencia de un evento tomando el número de formas en que el evento puede suceder dividiéndolo por el número total de resultados posibiles.\n\\[\nP(Evento) = \\frac{\\mbox{\\# de formas en que puede ocurrir el evento}}{\\mbox{\\# total de resultados posibles}}\n\\]\nPor ejemplo, si lanzamos una moneda, puede caer en cara o cruz. Para obtener la probabilidad de que la moneda caiga en cara, dividimos la forma de obtener cara entre los dos resultados posibles, cara y cruz.\n\\[\nP(cara) = \\frac{\\mbox{1 forma de obtener cara}}{\\mbox{2 posibles resultados}} = \\frac{1}{2} = fbox{50\\%}\n\\]\nLa probabilidad siempre está entte cero y 100% por ciento. Si la probabilidad de algo es cero, es imposible, y si la probabilidad de algo es del 100%, ciertamente sucederá."
  },
  {
    "objectID": "summary.html#asisgnación-de-vendedores",
    "href": "summary.html#asisgnación-de-vendedores",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.2 Asisgnación de Vendedores",
    "text": "2.2 Asisgnación de Vendedores\nVeamos un escenaro más complejo. Se acerca una reunión con un cliente potencial y queremos enviar a alguien del equipo de ventas a la reunión. Pondremos el nombre de cada persona en un boleto en una caja y sacaremos uno al azar para decidir quién va a reunión.\n\nEl nombre de Brian se saca.\n\nLa probabilidad de que Brian sea seleccionado es una de cuatro, o 25%. Esto es\n\\[\nP(Brian) = \\frac{1}{4} = \\fbox{25%}\n\\]\n\n2.2.1 Muestreo desde un marco de datos\nPodemos recrear este escenario en R usando la función sample_n de dplyr, que toma un marco de datos y la cantidad de filas que queremos extraer, que es solo 1 en este caso.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsales_counts &lt;- data.frame(name = c(\"Amir\", \"Brian\", \"claire\", \"Damian\"), n_sales = c(178, 126, 75, 69))\n\nsales_counts %&gt;% \n  sample_n(1)\n\n  name n_sales\n1 Amir     178\n\n\nSin embargo, si ejecutamos lo mismo nuevamente, es posible que obtengamos una fila diferente ya que sample_n elige al azar.\n\nsales_counts %&gt;% \n  sample_n(1)\n\n  name n_sales\n1 Amir     178\n\n\nSi queremos mostrarle al aquipo cómo elegimos a Brian, esto no funcionará bien.\n\n\n2.2.2 Establecer una semilla aleatoria\nPara asegurarnos de obtener los mismos resultados cuando ejectuamos el script frente al equipo, configuremos la semilla aleatoria usando set.seed(). La semilla es un número que el generador de números aleatorios de R usa como punto de partida, por lo que si lo orientamos con un número semilla, generará el mismo valor aleatorio cada vez. El número en si no importa. Podríamos usar 5,139 o 3 millones. Lo único que importa es que usemos la misma semilla la próxima vez que ejecutemos el script. Ahora, nosotros, o uno de los miembros del equipo de ventas, podemos ejecutar este código una y otra vez y obtener el mismo resultado de Brian cada vez.\n\nset.seed(5)\nsales_counts %&gt;% \n  sample_n(1)\n\n   name n_sales\n1 Brian     126\n\n\n\n\n2.2.3 Un segundo encuentro\nAhora hay otro cliente potencial que quiere reunirse al mismo tiempo, por lo que debemos elegir a otro vendedor. Brian ya ha sido elegido y no puede estar en dos reuniones a la vez, así que elegiremos entre las tres restantes. A esto se le llama muestro sin reemplazo, ya que no estamos reemplazando el nombre que ya sacamos. Esta vez, se elegi a Claire,\n\ny la probabilidad de que estos suceda es una de cada tres, o al rededor del 33%. Esto es,\n\\[\nP(Claire) = \\frac{1}{3} = \\fbox{33%}\n\\]\nPara recrear esto en R, podemos pasar 2 a sample_b, lo quie nos dará filas.\n\nset.seed(5)\nsales_counts %&gt;% \n  sample_n(2)\n\n    name n_sales\n1  Brian     126\n2 claire      75\n\n\nAhora digamos que las dos reuniones se realizan en diás diferentes, por lo que la misma persona podría asistir a ambas. En este escenario, debemos devolver el nombre de Brian a la caja después de elegirla. Esto se llama muestreo con reemplazo. Claire es elegida para la segunda reunión\n\nPero esta vez, la probabilidad es del 25%.\n\\[\nP(Claire) = \\frac{1}{4} = \\fbox{25%}\n\\]\nPara hacer esto en R, establezca el argumento de reemplazo de sample_n en TRUE.\n\nset.seed(5)\nsales_counts %&gt;% \n  sample_n(2, replace = TRUE)\n\n    name n_sales\n1  Brian     126\n2 claire      75\n\n\n\n\n2.2.4 Eventos Independientes\nHablemos rapidamente de la independencia. Dos eventos son independientes si la probabilidad del segundo evento no se ve afectada por el resultado del primero. Por ejemplo, si estamos muestreando con reemplazo, la probabilidad que Claire sea elegida en segundo lugar es del 25%, sin importar quien sea elegido primero. En general, al muestrear con reemplazo, cada selección es independiente.\n\nDe manera similar, los eventos se consideran dependientes cuando el resultado del primero cambia la probabilidad del segundo. Si muestreamos sin reemplazo, la probabilidad de que Claire sea elegida en segundo lugar depende de quién sea elegido primero. Si Claire es elegida primero, hay 0% de probabilidad de que Claire sea elegida en segundo lugar.\n\nEn general, cuando se muestrea sin reemplazo, cada selección es dependiente.\nEl siguiente cuadro comparativo contiene algunos ejemplos sobre muestreo con reemplazo y sin reemplazo.\n\n\n\n\n\n\n\nCon Reemplazo\nSin Reemplazo\n\n\n\n\nLanzar una moneda 3 veces\nDe una baraja de cartas, repartiendo a 3 jugadores 7 cartas cada un.,\n\n\nTirar un dado dos veces\nSeleccionar al azar 5 productos de la linea de montaje para probar el control de calidad\n\n\n\nElegir al azar a 3 personas para trabajar el fin de semana de un grupo de 20 personas"
  },
  {
    "objectID": "summary.html#ejemplo-6",
    "href": "summary.html#ejemplo-6",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.3 Ejemplo 6",
    "text": "2.3 Ejemplo 6\nSuponga que está a cargo del equipo de ventas y es hora de realizar revisiones de desempeño, comenzando con Amir. Como parte de la revisión, desea seleccionar al azar algunas de las ofertas en las que ha trabajado durante el último año para que pueda verlas más a fondo. Antes de comenzar a seleccionar ofertas, primero averiguará cuáles son las probabilidades de seleccionar ciertas ofertas.\nLa data que se creo a continuación, contiene la cantidad de tratos en los que Amir trabajo para cada tipo de producto.\n\namir_deals &lt;- data.frame(product = c(\"Product A\", \"Product B\", \"Product C\", \"Product D\", \"Product E\", \"Product F\", \"Product G\", \"Product H\", \"Product I\", \"Product J\", \"Product N\"), n = c(23, 62, 15, 40, 5, 11, 2, 8, 7, 2, 3))\n\nAhora, creamos una nueva columna llamada prob dividiendo n por el número total de tratos en los que trabajó Amir.\n\namir_deals %&gt;%\n  mutate(prob = n/sum(n))\n\n     product  n       prob\n1  Product A 23 0.12921348\n2  Product B 62 0.34831461\n3  Product C 15 0.08426966\n4  Product D 40 0.22471910\n5  Product E  5 0.02808989\n6  Product F 11 0.06179775\n7  Product G  2 0.01123596\n8  Product H  8 0.04494382\n9  Product I  7 0.03932584\n10 Product J  2 0.01123596\n11 Product N  3 0.01685393\n\n\nSi selecciona al azar una de las ofertas de Amir, ¿cuál es la probabilidad de que la oferta involucre el Producto C?\nRespuesta: 8.43%"
  },
  {
    "objectID": "summary.html#distribuciones-discretas",
    "href": "summary.html#distribuciones-discretas",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.4 Distribuciones Discretas",
    "text": "2.4 Distribuciones Discretas\nEn esta lección, profundizaremos en la probabilidad y comenzaremos a observar las distribuciones de probabilidad.\nConsideremos lanzar un dado estándar de seis caras.\n\nHay seis números, o seis resultados posibles, y cada número tiene exactamente una sexta parte, o alrededor de un 17 por ciento de posibilidades de salir. Este es un ejemplo de una distribución de probabilidad.\nEsto es similar al escenario anterior, excepto que teneiamos nombres en lugar de números.\n\nAl igual que tirar un dado, cada resultado o nombre tenía la misma probabilidad de ser elegido.\n\n2.4.1 Distribución de Probabilidad\n\nUna distribución de probabilidad describe la probabilidad de cada resultado posible en un escenario.\n\n2.4.1.1 Valor esperado\nTambién podemos hablar del valor esperado de una distribución, que es la media de una distribución.\nPodemos calcular esto multiplicando cada valor por su probabilidad y sumando, por lo que el valor esperado de lanzar un dado justo es 3.5. En el caso de los dados sera:\n\\[\n(1\\times \\frac{1}{6}) + (2\\times \\frac{1}{6}) + (3\\times \\frac{1}{6}) + (4\\times \\frac{1}{6}) + (5\\times \\frac{1}{6}) + (6\\times \\frac{1}{6}) = \\fbox{3.5}\n\\]\nPodemos visualizar esto usando un gráfico de barras, donde cada barra representa un resultado, y la altura de cada barra representa la probabilidad de ese resultado.\n\n\n\n2.4.1.2 Probabilidad = área\nPodemos calcular las probabilidades de diferentes resultados tomando áreas de la distribución de probabilidad. Por ejemplo, ¿cuál es la probabilidad de que nuestra tirada sea menor o igual a 2?\n\\[\n¿P(\\mbox{tirada dado}) \\leq 2?\n\\]\nPara resolver esto, tomaremos el área de cada barra que representa un resultado de 2 o menos. Cada barra tiene un ancho de 1 y una altura de 1/6, por lo que el área de cada barra es un sexto. Sumamos las áreas de 1 y 2 como se muestra en el gráfico para obtener una probabilidad total de \\(1/3\\).\n\n\n\n2.4.1.3 Dado desigual\n\nAhora digamos que tenemos un dado donde los dos se convirtieron en tres. Esto significa que ahora tenemos un 0% de posibilidades de obtener un 2 y un 33% de posibilidad de obtener un 3. Matemáticamente esto es\n\\[\n(1\\times 1/6) + (1\\times 0) + (3\\times 1/3) + (4\\times 1/6) + (5\\times 1/6) + (6\\times 1/6) = \\fbox{3.67}\n\\]\nya que es imposible obtener un 2 y un 3 por su nueva probabilidad, un tercio. Esto nos da un valor esperado que es ligeramente más alto que el dado justo.\n\n\n\n2.4.2 Visualización de Probabilidades desiguales\n\nCuando visualizamos estas nuevas probabilidades, las barras ya no son pares. Con este dado, ¿cuál es la probabilidad de obtener algo menor o igual a 2?, es decir\n\\[\nP(dado desigual) \\leq 2 = ?\n\\]\n\nHay un sexto de probabilidad de obtener 1 y cero probabilidad de obtener 2, que suma un sexto, tal como puede ver en la figura previa.\nLas distribuciones de probabilidad que ha visto hasta ahora son discretas, ya que representan situaciones con resultados discretos. Recuerde del capítulo anterior que las variables discretas pueden considerarse como variables contables o contadas. En el caso de un dado, estamos contando puntos, por lo que no podemos sacar 1.5 o 4.3."
  },
  {
    "objectID": "summary.html#distribución-uniforme",
    "href": "summary.html#distribución-uniforme",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.5 Distribución Uniforme",
    "text": "2.5 Distribución Uniforme\nCuando todos los resultados tienen la misma probabilidad, como un dado justo, se trata de una distribución especial denominada distribución uniforme discreta."
  },
  {
    "objectID": "summary.html#muestreo-a-partir-de-distribuciones-discretas",
    "href": "summary.html#muestreo-a-partir-de-distribuciones-discretas",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.6 Muestreo a partir de distribuciones discretas",
    "text": "2.6 Muestreo a partir de distribuciones discretas\nAl igual que tomamos muestras de nombres de una caja, podemos hacer lo mismo con distribuciones de probabilidad como las que hemos visto. Aquí hay un marco de datos llamado dado que representa un dado justo y su valor esperado es 3.5.\n\ndado &lt;- data.frame(n = c(1, 2, 3, 4, 5, 6))\n\nmean(dado$n)\n\n[1] 3.5\n\n\nTomaremos muestras de él 10 veces para simular 10 rollos. Tenga en cuenta que muestreamos con reemplazo para que estemos muestreando de la misma distribución cada vez.\n\nset.seed(4)\nrolls_10 &lt;- dado %&gt;% \n  sample_n(10, replace = TRUE)\nrolls_10\n\n   n\n1  3\n2  3\n3  3\n4  4\n5  3\n6  6\n7  5\n8  2\n9  3\n10 6\n\n\n\n2.6.1 Visualización de la muestra\nPodemos visualizar los resultados de los diez lanzamientos usando un histograma, estableciendo el número de contenedores en 6 ya que hay 6 resultados posibles.\n\nlibrary(ggplot2)\nggplot(rolls_10, aes(n)) + \n  geom_histogram(bins = 6)"
  },
  {
    "objectID": "summary.html#distribución-muestral-vs-distribución-teórica",
    "href": "summary.html#distribución-muestral-vs-distribución-teórica",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.7 Distribución muestral vs distribución teórica",
    "text": "2.7 Distribución muestral vs distribución teórica\nObserve que tenemos diferentes números de 1,2,3, etc., ya que la muestra fue aleatoria, aunque cada tirada teníamos la misma probabilidad de sacar cada una.\nLa media de nuestra muestra es de 3.3, que no esta tan cerca del 3.5 que esperabamos.\n\nmean(rolls_10$n)\n\n[1] 3.8\n\n\n\nmean(dado$n)\n\n[1] 3.5\n\n\n\n2.7.1 Una muestra más grande\nSi lanzamos el dado 100 veces, la distribución de las tiradas se ve un poco más uniforme y la media esta más cerca de 3.5\n\nset.seed(1)\nrolls_100 &lt;- dado %&gt;% \n  sample_n(100, replace = TRUE)\n\nmean(rolls_100$n)\n\n[1] 3.52\n\n\n\nggplot(rolls_100, aes(n)) + \n  geom_histogram(bins = 6)\n\n\n\n\n\n\n2.7.2 Ley de los grandes números\nEsto se llama la ley de los grandes números, que es la idea de que a medida que aumenta el tamaño de la muestra, la media de la muestra se acercará a la ,media teórica."
  },
  {
    "objectID": "summary.html#ejercicio-7",
    "href": "summary.html#ejercicio-7",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.8 Ejercicio 7",
    "text": "2.8 Ejercicio 7\nEn este ejercicio vamos a crear una distribución de probabilidad.\nSuponga que un nuevo restaurante abrió hace unos meses y la gerencia del restaurante quiere optimizar su espacio para sentarse en función del tamaño de los grupos que vienen con más frecuencia. En una noche, hay 10 grupos de personas esperando para sentarse en el restaurante, pero en lugar de ser llamados en el orden que llegaron, serán llamados al azar. Creemos hipoteticamente nuestra data\n\nrestaurant_groups &lt;- data.frame(group_id = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"), group_size = c(2, 4, 6, 2, 2, 2, 3, 2, 4, 2))\n\nEn este ejercicio, investigará la probabilidad de que grupos de diferentes tamaños de cada uno de los diez grupos están contenidos en el marco de datos restaurant_groups.\nPrimero cree un histograma de la columna group_size de restaurant_groups, estableciendo el número de contenedores en 5.\n\nggplot(restaurant_groups, aes(group_size)) +\n  geom_histogram(bins = 5)\n\n\n\n\nAhora, cuente el número de cada uno de los group_size en restaurant_groups, luego agregue una columna llamada probability que contiene la probabilidad de seleccionar aleatoriamente un grupo de cada tamaño. Almacene esto en un nuevo marco de datos llamado size_distribution.\n\nsize_distribution &lt;-restaurant_groups %&gt;%\n  # contamos el número de cada tamaño de grupo\n  count(group_size) %&gt;%\n  # Calculamos las probabilidades\n  mutate(probability = n / sum(n))\n\nsize_distribution\n\n  group_size n probability\n1          2 6         0.6\n2          3 1         0.1\n3          4 2         0.2\n4          6 1         0.1\n\n\nAhora, dado que ya tenemos las probabilidades, estamos listos para calcular el valor esperado de size_distribution, que representa el tamaño esperado del grupo.\n\nexpected_val &lt;- sum(size_distribution$group_size * size_distribution$probability)\nexpected_val\n\n[1] 2.9\n\n\nPor último, calculamos la probabilidad de elegir aleatoriamente un grupo de 4 o más personas filtrando y resumiendo.\n\nsize_distribution %&gt;%\n  # Filtramos para grupos de 4 o mas \n  filter(group_size &gt;= 4) %&gt;%\n  # Calculamos la prob_4_o_mas tomando la suma de probabilidades\n  summarize(prob_4_or_more = sum(probability))\n\n  prob_4_or_more\n1            0.3\n\n\n\n2.8.1 Indentificación de distribuciones\n¿Qué muestra es más probable que se haya tomado de una distribución uniforme?\n\nRespuesta: Opción B"
  },
  {
    "objectID": "summary.html#distribuciones-continuas",
    "href": "summary.html#distribuciones-continuas",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.9 Distribuciones Continuas",
    "text": "2.9 Distribuciones Continuas\nPodemos usar distribuciones discretas para modelar situaciones que involucran variables discretas o contables, pero ¿cómo podemos modelas variables continuas?\nComencemos con un ejemplo.\n\nEl autobús de la ciudad llega cada doce minutos, por lo que si te presentas a una hora aleatoria, que podrías esperar desde 0 minutos si llegas justo cuando llega el autobús, hasta 12 minutos si llegas cuando el autobus sale.\n\n2.9.1 Distribución uniforme continua\n\nModelemos este escenario con una distribución de probabilidad. Hay una cantidad infinita de minutos que podríamos esperar, ya que podríamos esperar 1 minuto, 1.5 minutos, 1.53 minutos, etc., por lo que no podemos crear bloques individuales como la hariamos con una variable discreta.\nEn cambio, usaremos una línea continua para representar la probabilidad. La linea es plana ya que existe la misma probabilidad de esperar cualquier tiempo de 0 a 12 minutos. Esto se llama la distribución uniforme continua.\n\nAhora que tenemos nuestra distribución, averiguemos cual es la probabilidad de que esperemos entre 4 y 7 minutos. Al igual que con las distribuciones discretas , podemos tomar el área de 4 a 7 para calcular la probabilidad.\n\\[\nP(4\\leq \\mbox{tiempo de espera} \\leq 7) = ?\n\\]\n\nAl igual que con las distribuciones discretas, podemos tomar el área de 4 a 7 para calcular la probabilidad.\n\nEl ancho de este rectángulo es 7 menos 4 que es 3. La altura es 1/12, entonces\n\\[\nP(4\\leq \\mbox{tiempo espera}\\leq 7) = 3\\times \\frac{1}{12} = \\frac{1}{4} = \\fbox{25\\%}\n\\]\n\n\n2.9.2 Distribución Uniforme en R\nUsemos la distribución uniforme en R para calcular la probabilidad de esperar 7 minutos o menos. Pasaremos 7 a punif función que nos ayuda a calcular la probabilidad uniforme en R. Veamos:\n\npunif(7, min = 0, max = 12)\n\n[1] 0.5833333\n\n\nPor tanto, la probabilidad de esperar menos de 7 minutos es del \\(\\fbox{58\\%}\\).\n\n\n2.9.3 Cola Inferior\nSi queremos la probabilidad de esperar más de 7 minutos,\n\\[P(\\mbox{tiempo de espera} \\geq 7) = ?\\]establezca el argumento de cola de punto inferior en FALSO.\n\nEn R esto es:\n\npunif(7, min = 0, max = 12, lower.tail = FALSE)\n\n[1] 0.4166667\n\n\nPero, ¿cómo calculamos la probabilidad de esperar de 4 a 7 minutos usando R?\n\nPodemos empezar con la probabilidad de esperar menos de 7 minutos, luego reste la probabilidad de esperar menos de 4 minutos, tal como puede apreciar en la figura. En R esto es:\n\npunif(7, min = 0, max = 12) - punif(4, min = 0, max = 12)\n\n[1] 0.25\n\n\nPara calcular la probabilidad de esperar entre 0 y 12 minutos\n\\[\nP(0\\leq \\mbox{tiempo de espera}\\leq 12) = ?\n\\]\nGraficamente esto es\n\nPor tanto, no es mas que multiplicar 12 por 1/2, es decir:\n\\[\nP(0\\leq \\mbox{tiempo de espera}\\leq 12) = 12\\times \\frac{1}{12} = \\fbox{1}\n\\]\nEsto tiene sentido ya que estamos eguros que esperaremos entre 0 y 12 minutos.\n\n\n2.9.4 Otras Distribuciones Continuas\n\nLas distribuciones continuas pueden tomar formas distintas a las uniformes donde algunos valores tienen una probabilidad mayor que otros. No importa la forma de la distribución, el área debajo de ella siempre debe de ser igual a 1.\n\nEsto también se aplicará a otras distribuciones que aprenderemos más adelante en este curso, como la distribución normal o distribución de Poisson, que se puede usar para modelar muchas situaciones de la vida real."
  },
  {
    "objectID": "summary.html#ejercicio-8",
    "href": "summary.html#ejercicio-8",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.10 Ejercicio 8",
    "text": "2.10 Ejercicio 8\nEn este punto, ha aprendido acerca de las dos variantes diferentes de la distribución uniforme: la distribución uniforme discreta y la distribución uniforme continua. en este ejercicio, decidirá qué situaciones siguen qué distribución.\n\nAsignaremos cada situación a la distribución de probabilidad con la que se modelaría mejor.\n\n\n\n\n\n\n\n\nUniforme discreto\nUniforme continuo\nOtro\n\n\n\n\nEl número de boleto de un ganador de la rifa, suponiendo que haya un boleto para cada número del 1 al 100.\nLa hora del día en que nacerá un bebé.\nLa altura de una persona al azar.\n\n\nEl resultado de lanzar un dado de 4 caras.\nEl tiempo que tendrás que esperar para que un géiser entre en erupción si te presentas en un momento aleatorio, sabiendo que el géiser entra en erupción exactamente cada diez minutos."
  },
  {
    "objectID": "summary.html#ejercicio-9",
    "href": "summary.html#ejercicio-9",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.11 Ejercicio 9",
    "text": "2.11 Ejercicio 9\nEl software de ventas utilizado en su empresa está configurado para realizar copias de seguridad automaáicamente, pero nadie sabe exactamente a qué hora se realizan las copias de seguridad. Sin embargo, se sabe que las copias de seguridad se realizan exactamente cada 30 minutos. Amir regresa de las reuniones de ventas en momentos aleatorios para actualizar los datos del cliente con el que acaba de reunirse. Quiere saber cuánto tiempo tendrá que esperar para que realice una copia de seguridad de sus datos recién ingresados. Usando nuestro nuevo conocimiento de distribuciones uniformes continuas para modelar esta situación y responder las preguntas de Amir.\n\nPrimero, para modelar cuánto tiempo esperará Amir por una copia de seguridad utilizando una distribución uniforme continua, guarde su tiempo de espera más bajo posible como min y su tiempo de espera más alto posible como max. Recuerde que las copias de seguridad se realizan cada 30 minutos.\n\n# Tiempos de espera mínimos y máximos para la copia de seguridad que se realiza cada 30 minutos\nmin &lt;- 0\nmax &lt;- 30\n\nCalculemos la probabilidad de que Amir tenga que esperar menos menos de 5 minutos y guárdala en una nueva variable llamada prob_less_than_5.\n\nprob_less_than_5 &lt;- punif(5, min = min, max = max)\nprob_less_than_5\n\n[1] 0.1666667\n\n\nAhora, calculemos la probabilidad de que Amir tenga que esperar más de 5 minutos y guárdala en una variable llamada prob_greater_than_5.\n\nprob_greater_than_5 &lt;- punif(30, min = min, max= max)- punif(5, min = min, max = max)\nprob_greater_than_5\n\n[1] 0.8333333\n\n\nPor último calcule la probabilidad de que Amir tenga que esperar entre 10 y 20 minutos, y guárdala en una nueva variable llamada prob_between_10_end_20.\n\nprob_between_10_and_20 &lt;- punif(20, min = min, max = max) - punif(10, min = min, max = max)\nprob_between_10_and_20\n\n[1] 0.3333333"
  },
  {
    "objectID": "summary.html#ejercicio-10",
    "href": "summary.html#ejercicio-10",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.12 Ejercicio 10",
    "text": "2.12 Ejercicio 10\nPara darle a Amir una mejor idea de cuánto tiempo tendrá que esperar, simularemos la esperá de Amir 1000 veces y creará un histograma para mostrarle lo que debe esperar. Recuerde del último ejercicio que su tiempo de espera mínimo es de 0 minutos y su tiempo de espera máximo es de 30 minutos.\nCreemos un marcos de datos llamado wait_times y establezcamos un valor semilla de 334.\n\nset.seed(334)\nwait_times &lt;- data.frame(simulation_nb = c(seq(1:1000)))\nhead(wait_times)\n\n  simulation_nb\n1             1\n2             2\n3             3\n4             4\n5             5\n6             6\n\n\nAhora, generamos 1000 tiempos de espera a partir de la distribución uniforme continua que modela el tiempo de espera de Amir. Agregue esto como una nueva columna llamada time en el marco de datos wait_times.\n\nwait_times_1000 &lt;- wait_times %&gt;%\n  mutate(time = runif(1000, min = 0, max = 30))\n\nLuego, creamos el histograma de los tiempos de espera simulados con 30 contenedores.\n\nwait_times_1000 %&gt;%\n  mutate(time = runif(1000, min = 0, max = 30)) %&gt;%\n  ggplot(aes(time)) +\n  geom_histogram(bins = 30)\n\n\n\n\nExcelente simulación! A menos que Amir descubra exactamente a qué hora ocurre cada copia de seguridad, no podrá programar la entrada de datos para que se haga una copia de seguridad antes, pero parece que esperará unos 15 minutos en promedio."
  },
  {
    "objectID": "summary.html#distribución-binomial",
    "href": "summary.html#distribución-binomial",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.13 Distribución Binomial",
    "text": "2.13 Distribución Binomial\nEs hora de ampliar aún más su caja de herramientas de distribuciones. En este sección, aprenderá sobre la distribución bonomial.\n\n2.13.1 Lanzamiento de monedas\n\nComenzaremos lanzando una moneda, que tiene dos resultados posibles, cara o cruz, cada uno con una probabilidad del 50%.\n\n\n2.13.2 Resultados Binarios\n\nEste es un ejemplo de un resultado binario, o un resultado con dos valores prosibles. También podríamos respresentar estos resultados como un 1 y un 0, un éxito o un fracaso, y una victoria o una derrota.\nEn R, podemos simular esto usando la función rbinom, que toma en cuenta la cantidad de intentos o veces que queremos lanzar, la cantidad de monedas que queremos lanzar y la probabilidad de cara o éxito. Esto devolverá un 1 que contaremos como cara o un 0 que contaremos como cruz. Podemos usar\n\nrbinom(1, 1, 0.5)\n\n[1] 1\n\n\nPara realizar ocho lanzamientos de moneda, podemos cambiar el primer argumento un 8, lo que nos dará ocho lanzamientos de una moneda con un 50% de prosibilidades de cara. Esto nos da un conjunto de 8 unos y ceros.\n\nrbinom(8, 1, 0.5)\n\n[1] 0 0 0 0 0 0 0 1\n\n\n\n\n2.13.3 Muchas volteretas una vez\nSi intercambiamos los dos primeros argumentos, simulamos un lanzamiento de 8 monedas. Esto nos da un número, que es el número total de caras o éxitos.\n\nrbinom(1, 8, 0.5)\n\n[1] 4\n\n\n\n\n2.13.4 Muchas volteretas muchas veces\nDe manera similar, podemos pasar 10 y 3 a rbinom para simular 10 lanzamientos de 3 monedas. Esto devuelve 10 números, cada uno de los cuales representa el número total de caras de cada conjunto de lanzamientos.\n\nrbinom(10, 3, 0.5)\n\n [1] 0 0 2 2 2 1 3 1 1 1\n\n\n\n\n2.13.5 Otras probabilidades\nTambién podríamos tener una moneda que sea más pesada en un lado que en el otro, por lo que la probabilidad de obtener cara es solo del 25%.\n\nPara simular lanzamientos con esta moneda, ajustaremos el tercer argumento de rbinom a 0.25. El resultado tiene números más bajos, ya que no es tan probable obtener varias caras con la nueva moneda.\n\nrbinom(10, 3, 0.25)\n\n [1] 1 1 0 0 0 0 1 1 1 2"
  },
  {
    "objectID": "summary.html#distribución-binomial-1",
    "href": "summary.html#distribución-binomial-1",
    "title": "2  Números aleatorios y Probabilidad",
    "section": "2.14 Distribución Binomial",
    "text": "2.14 Distribución Binomial\nLa distribución binomial describle la probabilidad del número de éxitos en una secuencia de ensayos independientes. En otras palabras, puede decirnos la probabilidad de obtener cierto número de caras en una secuencia de lanzamientos de monedas. Tenga en cuenta que esta es una distribución discreta ya que estamos trabajando con un resultado contable. La distribución binomial se puede describir usando dos paramétros n y p.\n\nn representa el número total de ensayos que se están realizando.\np representa la probabilidad de los ensayos\nn y p son también el segundo y tercer argumento de rbinom.\n\nAsí es como se ve la distribución de 10 monedas. Tenemos la mayor probabilidad de obtener 5 caras en total, y una posibilidad mucho menor de obtener 0 caras o 10 caras.\n\n\n2.14.1 ¿Cuál es la probabilidad de 7 caras?\n\\[\nP(caras = 7)?\n\\]\nPara obtener la probabilidad de obtener 7 caras de 10 monedas, podemos usar dbinom().\n\nEl primer argumento es el número de caras o éxitos.\nEl segundo argumento es el número de intentos, n, y\nEl tercer agumento es la probabilidad de éxito p\n\nSi lanzamos 10 monedas, hay un 12% de probabilidad de que 7 de ellas sean cara.\n\ndbinom(7, 10, 0.5)\n\n[1] 0.1171875\n\n\n\n\n2.14.2 ¿Cuál es la probabilidad de obtener más de 7 caras?\n\\[\nP(cara &gt; 7)?\n\\]\nPodemos usar el argumento de cola de punto inferior para obtener la probabilidad de un número de éxitos mayor que el primer argumento. Tenga en cuenta que esto es lo mismo que 1 menos la misma llamada pbinom\n\npbinom(7, 10, 0.5, lower.tail = FALSE)\n\n[1] 0.0546875\n\n\n\n\n2.14.3 Valor Esperado\nEl valor esperado de la distribución binomial se puede calcular multiplicando n por el p.\n\\[\n\\bar{x}= n\\times p\n\\]\nEl número esperado de caras que obtendremos al lanzar 10 monedas es 10 veces 0.5 que es 5.\n\\[\n\\bar{x} = 10\\times 0.5 = \\fbox{5}\n\\]\n\n\n2.14.4 Independencia\nEs importante recordar que para que se aplique la distribución binomial, cada ensayo debe ser independiente, por lo que el resultado de un ensayo no debería afectar al siguiente. Por ejemplo, si elegimos al azar de estas tarjetas con ceros y unos\n\nTenemos una probabilidad de 50 - 50 de obtener un 0 un 1.\nPero como estamos muestreando sin reemplazo, las probabilidades para la segunda prueba son diferentes debido al resultado de la primera prueba.\n\nDado que estos ensayos no son independientes, no podemos calcular probabilidades precisas para esta situación utilizando la distribución binomial.\n\n\n2.14.5 Ejercicio 11\nSuponga que Amir generalmente trabaja en 3 tratos por semana y, en general, gana eñ 30% de los tratos en los que trabaja. Cada trato tiene un resultado binario: se pierde o se gana, por lo que puede modelar sus tratos de ventas con una distribución binomial. En este ejercicio, ayudará a Amir a simular un año de sus transacciones para que pueda comprender mejor su desempeño.\n\nPrimero, establezca una semilla aleatoria en 10 y simule un solo trato.\n\nset.seed(10)\nrbinom(1, 1, 0.3)\n\n[1] 0\n\n\nAhora, simule una semana típica de ofertas de Amir, o una semana de 3 ofertas.\n\nrbinom(1, 3, 0.3)\n\n[1] 0\n\n\nPor último, simule un año de ofertas de Amir, o 52 semanas de 3 ofertas cada una, y guárdelas en formato deals. Calcule el número medio de tratos que ganó por semana.\n\ndeals &lt;- rbinom(52, 3, 0.3)\n\nmean(deals)\n\n[1] 0.8076923\n\n\n\n\n\n2.14.6 Ejercicio 12\nAl igual que en el último ejercicio, suponga que A,mir gana el 30% de los tratos. Quiere tener una idea de la probabilidad de que cierre una cierta cantidad de tratos cada semana. En este ejercicio, calculará cuáles son las probabilidades de que cierre diferentes números de tratos utilizando la distribución binomial.\n\nPrimero, cuál es la probabilidad de que Amir cierre los 3 tratos en una semana?\n\n# Aquí utilizamos la función de dbinom() puesto que queremos sacar una sola probabilidad de masa, es decir 3 tratos en una semana \ndbinom(3, 3, 0.3)\n\n[1] 0.027\n\n\nEsto nos dice que de 3 ensayos que hacemos queremos ver las probabilidad de que esto ocurra el 100% de las veces, es decir, 3, teniendo en cuenta que la probabilidad de exito es del 30%.\n¿cuál es la probabilidad de que Amir cierre 1 trato o menos en una semana?\n\n# \n# sin embargo, en este caso, queremos sacar más probabilidades que en el caso anterior, es decir, antes solamente calculabamos la probabilidad de saldar 3 tratos en 1 semana, es decir una solo caso. \npbinom(1, 3, 0.3)\n\n[1] 0.784\n\n\nDicho de otra manera, queremos saber cual es la probabilidad de que el éxito, ocurra una vez en 3, teniendo en cuenta que la probabilidad es del 30%.\n¿Cuál es la probabilidad de que Amir cierre más de 1 trato?\n\npbinom(1, 3, 0.3, lower.tail = FALSE)\n\n[1] 0.216\n\n\nLo que estamos viendo en este caso, es ver cual es la probabilidad de tener por lo menos, 1 éxito, o más 2 o incluso 3, de tres ensayos.\n\n\n\n2.14.7 Ejercicio 13\nAhora amir quiere saber cuántos tratos puede esperar cerrar cada semana si cambia su tasa de ganancias. Afortunadamente, puede usar su conocimiento de distribución binomial para ayudarlo a calcular el valor esperado en diferentes situaciones.\n\nPrimero, calcule el número esperado de ventas de las 3 en las que trabaja que Amir ganará cada semana si mantiene su tasa de ganancia del 30%.\n\nwon_30pct &lt;- 3 * 0.3\nwon_30pct\n\n[1] 0.9\n\n\nSegundo, calcule el número esperado de ventas de las 3 en las que trabaja que ganará si su tasa de ganancias cae al 25%.\n\nwon_25pct &lt;- 3 *0.25\nwon_25pct\n\n[1] 0.75\n\n\nTercero, calcule la cantidad esperada de ventas de las 3 en las que trabaja que ganará si su tasa de ganancias aumenta al 35%.\n\nwon_35pct &lt;- 3 * 0.35\nwon_35pct\n\n[1] 1.05"
  },
  {
    "objectID": "intro.html#qué-es-la-estadística",
    "href": "intro.html#qué-es-la-estadística",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.1 ¿Qué es la estadística ?",
    "text": "1.1 ¿Qué es la estadística ?\nEs la práctica y el estudio de la recopilación y el análisis de datos. También podemos hablar de una estadística de resumen, que es un hecho o un resumen de algunos datos, como un promedio o un conteo.\n\n1.1.1 ¿Qué pueden hacer las estadísticas?\non el poder de las estadísticas, podemos responder toneladas de preguntas diferentes como: ¿Qué tan probable es que alguien compre un producto? ¿Es más probable que las personas lo compren si pueden usar un sistema de pago diferente? ¿Cuántos ocupantes tendrá su hotel? ¿Cómo se puede optimizar la ocupación? ¿Cuántas tallas de jeans deben fabricarse para que le queden al 95% de la población? ¿Se debe producir el mismo número de cada tamaño? Una pregunta como, ¿Qué anuncio es más eficaz para que la gente compre un producto? Se puede responder con pruebas A/B\n\n\n1.1.2 ¿Qué no pueden hacer las estadísticas?\nSi bien las estadísticas pueden responder muchas preguntas, es importante tener en cuenta que las estadísticas no pueden responder todas las preguntas. Si queremos saber por qué la serie de TV Game of Thrones es tan popular, podríamos preguntar a todos por qué les gusta, pero pueden mentir o dejar de lado las razones. Podemos ver si las series con escenas más violentas atraen a más espectadores, pero incluso si lo hacen, no podemos saber si la violencia en Game of Throne es la razón de su popularidad, o si otros factores están impulsando su popularidad y simplemente sucede.\n\n\n1.1.3 Tipos de Estadísticas\nHay dos ramas principales de la estadística:\n\nEstadística Descriptiva\nEstadística Inferencial\n\n\n1.1.3.1 Estadística Descriptiva\nEste tipo de estadística se centra en describir y resumir los datos disponibles. Después de preguntar a cuatro amigos cómo llegan al trabajo, podemos ver que el 50% de ellos conducen al trabajo, el 25% van en autobús y el 25% en bicicleta. Estos son ejemplos de estadísticas descriptivas.\n\n\n1.1.3.2 Estadística Inferencial\nEste tipo de estadística utiliza los datos disponibles, que se denominan datos de muestra, para hacer inferencia sobre una población más grande. Podríamos usar estadísticas inferenciasles para determinar que porcentaje de personas conducen al trabajo en función de nuestros datos de muestra.\n\n\n\n1.1.4 Tipos de Datos\nExisten dos tipos principales de datos. Los datos númericos o cuantitativos se componen de valores numéricos. Los datos categóricos o cualitativos están formados por valores que pertenecen a distintos grupos. Es importante tener en cuenta que estos no son los únicos tipos de datos que existen; también hay otros, pero nos centraremos en estos dos. Los datos numéricos se pueden separar en datos:\n\ncontinuos, y\ndiscretos.\n\nLos datos numéricos continuos suelen ser cantidades que pueden medir, como la velocidad o el tiempo. Los datos numéricos discretos suelen ser datos de conteo, como la cantidad de mascotas o la cantidad de paquetes enviados. Los datos categóricos pueden ser nominales o ordinales.\n\nLos datos categóricos nominales se componen de categorías sin orden inherente, como el estado civil o el país de residencia.\nLos datos categóricos ordinales tienen un orden inherente, como una una pregunta de encuesta donde debe indicar el grado en que está de acuerdo con una afirmación.\n\n\n\n1.1.5 Los datos categóricos se pueden presentar como números\nA veces, las variables categóricas se representan mediante números. Los casados y solteros se pueden representar usando 1 y 0, o una escala del 1 al 5. Sin embargo es importante tener en cuenta que esto no necesariamente los convierte en variables númericas.\n\n\n1.1.6 ¿Por qué es importante el tipo de dato?\nSer capaz de identificar los tipos de datos es importante, ya que el tipo de datos con los que está trabajando dictará qué tipos de estadística de resumen y visualizaciones tienen sentido para sus datos, por lo que esta es una habilidad importante que debe dominar. Para datos numericós, podemos usar estadísticas de resumen como la media y gráficos como diagramas de dispersión, pero estos no tienen mucho sentido para los datos categóricos.\nVeamos el siguiente ejemplo, para comenzar a diferenciar la estadística descriptiva de la inferencial\n\n\n\n\n\n\n\nDescriptiva\nInferencial\n\n\n\n\nTeniendo en cuenta los datos de cada solicitud de sevicio al cliente realizada, ¿Cuál es el tiempo promedio que se tardo en responder?\nDespués de entrevistar a 100 clientes, ¿Qué porcentaje de todos sus clientes está satisfecho con su producto?\n\n\nDados los datos de las 100,000 personas que vieron el anuncio ¿Qué porcentaje de personas hizo click en el?\nDados los datos de 20 peces capturados en un lago, ¿cuál es el peso promedio de todos los peces del lago?\n\n\n\nAhora, hagamos un ejercicio similar para identificar el tipo de dato.\n\n\n\n\n\n\n\n\nnumérico (continuo)\nnumérico (discreto)\ncategórico\n\n\n\n\nKilovatios de electricidad utilizados\nNúmero de cursos de estadística tomados\nMarca de un producto\n\n\nTemperatura del aire\nNúmero de artículos en Stock\nCódigo Postal\n\n\n\nNúmero de click en un anuncio"
  },
  {
    "objectID": "intro.html#medidas-del-centro",
    "href": "intro.html#medidas-del-centro",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.2 Medidas del Centro",
    "text": "1.2 Medidas del Centro\nEn esta sección, comenzaremos a analizar las estadísticas de resumen, algunas de las cuales quizás ya le resulten familiares, como la media y la mediana.\n\n1.2.1 Datos de sueño de mamiferos\nVeremos los datos sobre los hábitos de sueño de diferentes mamíferos. Estos datos ya estan precargados por defecto en R en el paquete o librería de ggplot2, veamos como acceder a ellos en R.\n\nlibrary(ggplot2)  \ndata(\"msleep\") \nmsleep\n\n# A tibble: 83 × 11\n   name   genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 Cheet… Acin… carni Carn… lc                  12.1      NA        NA      11.9\n 2 Owl m… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n 3 Mount… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n 4 Great… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n 5 Cow    Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n 6 Three… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n 7 North… Call… carni Carn… vu                   8.7       1.4       0.383  15.3\n 8 Vespe… Calo… &lt;NA&gt;  Rode… &lt;NA&gt;                 7        NA        NA      17  \n 9 Dog    Canis carni Carn… domesticated        10.1       2.9       0.333  13.9\n10 Roe d… Capr… herbi Arti… lc                   3        NA        NA      21  \n# ℹ 73 more rows\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\nAntes de sumerginos en esto, recordemos cómo funcionan los histogramas. Un histograma toma un montón de puntos de datos y los separa en contenedores o rangos de valores.\n\nAquí hay un contenedor de 0 a 2 horas, de 2 a 4 horas y así sucesivamente. Las alturas de las barras representan las cantidad de puntos de datos que caen en ese contenedor, por lo que hay un mamífero en el conjunto de datos que duerme entre 0 y 2 horas, y nueve mamíferos que duermen de dos a cuatro horas. Los histogramas son una excelente manera de resumir visualmente los datos, pero podemos usar estadísticas de resumen numérico para resumir aún más.\n\n\n1.2.2 ¿Cuánto tiempo suelen dormir los mamíferos de este conjunto de datos ?\n\nUna forma de resumir los datos es respondiendo a la pregunta de esta sección. Para responder a esto, necesitamos averiguar cuál es el valor típico o central de los datos. Discutiremos tres definiciones diferentes, o medidas de centro:\n\nMedia\nMediana\nModa"
  },
  {
    "objectID": "intro.html#medidas-del-centro-1",
    "href": "intro.html#medidas-del-centro-1",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.3 Medidas del Centro",
    "text": "1.3 Medidas del Centro\n\n1.3.1 Media\nLa media, a menudo llamada promedio, es una de las formas más comunes de resumir los datos. Para calcular la media, sumamos todos los números de interés y los dividimos por el número total de puntos de datos, que aquí es 83.\n\\[\nmedia = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\n\\]\nEsto nos da 10 punto 43 horas de sueño. En R, podemos usar la función mean() pasándole la variable de interés.\n\nmean(msleep$sleep_total)\n\n[1] 10.43373\n\n\n\n\n1.3.2 Mediana\nOtra medida de centro es la mediana. La mediana es el valor en el que el 50% de los datos es inferior y el 50% de los datos es superior. Podemos calcular esto ordenando todos los puntos de datos y tomando el del medio, que sería el indice 42 en este caso.\n\nsort(msleep$sleep_total)\n\n [1]  1.9  2.7  2.9  3.0  3.1  3.3  3.5  3.8  3.9  4.0  4.4  5.2  5.3  5.3  5.4\n[16]  5.6  6.2  6.3  6.3  7.0  7.7  8.0  8.3  8.4  8.4  8.6  8.7  8.7  8.9  9.1\n[31]  9.1  9.4  9.4  9.5  9.6  9.7  9.8  9.8 10.0 10.1 10.1 10.1 10.3 10.3 10.4\n[46] 10.6 10.9 11.0 11.0 11.1 11.3 11.5 12.1 12.5 12.5 12.5 12.5 12.8 12.8 13.0\n[61] 13.5 13.7 13.8 14.2 14.3 14.4 14.4 14.5 14.6 14.9 14.9 15.6 15.8 15.8 15.9\n[76] 16.6 17.0 17.4 18.0 18.1 19.4 19.7 19.9\n\n\nEsto nos da una mediana de 10.1 horas de sueño.\n\nsort(msleep$sleep_total)[42]\n\n[1] 10.1\n\n\nEn R, podemos usar la función median() para hacer los cálculos por nosotros.\n\nmedian(msleep$sleep_total)\n\n[1] 10.1\n\n\n\n\n1.3.3 La Moda\nLa moda es el valor más frecuente en los datos. Si contamos cuantas ocurrencias hay dentro de cada sleep_total y ordenamos en orden descendente, veamos\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmsleep %&gt;% count(sleep_total, sort = TRUE)\n\n# A tibble: 65 × 2\n   sleep_total     n\n         &lt;dbl&gt; &lt;int&gt;\n 1        12.5     4\n 2        10.1     3\n 3         5.3     2\n 4         6.3     2\n 5         8.4     2\n 6         8.7     2\n 7         9.1     2\n 8         9.4     2\n 9         9.8     2\n10        10.3     2\n# ℹ 55 more rows\n\n\nnote que hay 4 mamiferos que duermen durante 12.5 horas, así que esta es la moda. La moda de la variable vore del conjunto de datos, que indica la dieta del animal es herbívora, veamomos:\n\nmsleep %&gt;% count(vore, sort = TRUE)\n\n# A tibble: 5 × 2\n  vore        n\n  &lt;chr&gt;   &lt;int&gt;\n1 herbi      32\n2 omni       20\n3 carni      19\n4 &lt;NA&gt;        7\n5 insecti     5\n\n\nLa moda se usa a menudo para variables categóricas, ya que las variables categóricas, pueden estar desordenadas y, a menudo no tienen una representación numérica inherente."
  },
  {
    "objectID": "intro.html#agregar-un-valor-atípico",
    "href": "intro.html#agregar-un-valor-atípico",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.4 Agregar un valor atípico",
    "text": "1.4 Agregar un valor atípico\nAhora que tenemos muchas formas de medir el centro, ¿cómo sabemos cuál usar? Veamos un ejemplo. Aquí, tenemos todos los insectivoros en el conjunto de datos\n\nmsleep %&gt;% \n  filter(vore == \"insecti\") %&gt;% \n  summarize(mean_sleep = mean(sleep_total),\n            median_sleep = median(sleep_total))\n\n# A tibble: 1 × 2\n  mean_sleep median_sleep\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       14.9         18.1\n\n\nObtenemos un tiempo de sueño medio de 14.94 horas y un tiempo de sueño medio de 18.1 horas.\nAhora digamos que hemos descubierto un nuevo insectívoro misterioso que nunca duerme.\nSi volvemos a tomar la media y mediana, obtenemos resultados diferentes. La media se puede reducir en más de 3 horas, mientras que la mediana cambió en menos de una hora. Esto se debe a que la media es mucho más sensible a los valores extremos que la mediana.\nDado que la media es más sensible a los valores extremos, funciona mejor para datos simétricos como este.\n\nObserve que la media, en rojo, y la mediana en verde, estan bastante cerca."
  },
  {
    "objectID": "intro.html#sesgo",
    "href": "intro.html#sesgo",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.5 Sesgo",
    "text": "1.5 Sesgo\nSin embargo, si los datos estan sesgados, lo que significa que no son simétricos, como se muestra en la figura siguiente\n\ngeneralmente es mejor usar la mediana. En este histograma, note que los datos se apilan a la derecha. Los datos que se ven así se denominan datos asimétricos a la izquierda. Cuando los datos se acumulan a la izquierda, están sesgados a la derecha, vea la siguiente figura.\n\nCuando los datos están sesgados, la media y la mediana son diferentes. La media se tira en dirección del sesgo, por lo que es más baja que la mediana en los datos sesgados a la izquierda y más alta que la mediana en los datos sesgados a la derecha.\n\nDebido a que la media se ve afectada por los valores extremos, es mejor usar la mediana, ya que se ve menos afectada por los valores atípicos."
  },
  {
    "objectID": "intro.html#ejercicio-1",
    "href": "intro.html#ejercicio-1",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.6 Ejercicio 1",
    "text": "1.6 Ejercicio 1\nEn este capítulo, trabajaremos con el indice de huella de carbono alimentario 2018. El conjunto de datos food_consumption contiene información sobre los kilogramos de alimentos consumidos por persona por año en cada país en cada categoría de alimentos (consumption), así como información sobre la huella de carbono de esa categoría de alimentos (co2_emissions) medida en kilogramos de dióxido de carbono o \\(CO_2\\), por persona por año en cada país.\nEn este ejercicio, nos encargaremos de calcular medidas de centro para comparar el consumo de alimentos en EE.UU y Bélgica.\n\nPrimero creamos dos marcos de datos: uno que contenga las filas de food_consumption para Bélgica y otro que contenga las filas de USA. Llamaremos a estos marcos de datos como belgica_consumption y usa_consumption. Veamos como hacerlo en R.\n\n\n# Primero cargamos el conjunto de datos\nfood_consumption &lt;- readRDS(\"food_consumption.rds\")\n\nbelgica_consumption &lt;- food_consumption %&gt;% \n  filter(country == \"Belgium\")\n\nusa_consumption &lt;- food_consumption %&gt;% \n  filter(country == \"USA\")\n\nAhora, calculamos la media y mediana de kilogramos de alimentos consumidos por persona por año para ambos países.\n\nBélgica\n\n\nmean(belgica_consumption$consumption)\n\n[1] 42.13273\n\n\n\nmedian(belgica_consumption$consumption)\n\n[1] 12.59\n\n\n\nUsa\n\n\nmean(usa_consumption$consumption)\n\n[1] 44.65\n\n\n\nmedian(usa_consumption$consumption)\n\n[1] 14.58\n\n\nPo último, filtremos food_consumption por filas con datos sobre Bélgica y EE.UU. Agruparemos los datos filtrados por país (country). Calcularemos la media y mediana de los kilogramos de alimentos consumidos por persona al año en cada país. Llame a estas columnas mean_consumption y median_consumption. Veamos:\n\nfood_consumption %&gt;%\n  # Filtramops para Belgium and USA\n  filter(country %in% c(\"Belgium\", \"USA\")) %&gt;%\n  # agrupamos por pais (country)\n  group_by(country) %&gt;%\n  # Obtenemos resumen de consumo medio y mediano.\n  summarise(mean_consumption = mean(consumption),\n      median_consumption = median(consumption))\n\n# A tibble: 2 × 3\n  country mean_consumption median_consumption\n  &lt;chr&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1 Belgium             42.1               12.6\n2 USA                 44.6               14.6"
  },
  {
    "objectID": "intro.html#ejercicio-2",
    "href": "intro.html#ejercicio-2",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.7 Ejercicio 2",
    "text": "1.7 Ejercicio 2\nEn esta sección, usted aprendio que la media es la suma de todos los puntos de datos dividida por el número total de puntos de datos , y que la mediana es el valor medio del conjunto de datos donde el 50% de los datos es menor que la mediana y 50% de los datos es mayor que la mediana. En este ejercicio, comparará estas dos medidas del centro.\nPara ello usaremos el paquete ggplot2 que nos facilita la creación de gráficos.\nPrimero, filtramos food_consumption para obtener las filas donde food_category está rice.\nSegundo, cree un histograma usando ggplot2 de co2_emission para el arroz (rice)\n\nfood_consumption %&gt;%\n  # Filter for rice food category\n  filter(food_category == \"rice\") %&gt;%\n  # Create histogram of co2_emission\n  ggplot(aes(co2_emission)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMire el histograma para el arroz (rice) que acabamos de calcular. ¿Cuál de los siguientes terminos describe mejor los datos?\n\nSin sesgo\nSesgado a la izquierda\nSesgado a la derecha\n\nViendo el gráfico, claramento los datos están sesgado a la derecha.\nEn este mismo ejercicio hagamos algo más. Filtremos food_consumption para obtener las filas donde food_category está rice.\nLuego, resumiremos los datos para obtener la media y la mediana de co2_emission, llamándolos mean_co2 y median_co2.\n\nfood_consumption %&gt;%\n  # Filtramos por categoría de alimentos de arroz\n  filter(food_category == \"rice\") %&gt;% \n  # obtenemos mean_co2 and median_co2\n  summarize(mean_co2 = mean(co2_emission),\n            median_co2 = median(co2_emission))\n\n# A tibble: 1 × 2\n  mean_co2 median_co2\n     &lt;dbl&gt;      &lt;dbl&gt;\n1     37.6       15.2\n\n\nDada la asimetría de estos datos, ¿Qué medida de tendencia central resume mejor los kilogramos de \\(CO_2\\) emisiones por persona por año para el arroz?\n\nMedia\nMediana\nAmbos la media y la mediana\n\nViendo la asimetría de los datos de nuestro último histograma, la medida de tendencia central que mejor resume los kilogramos de CO2 emisiones por persona será o es la mediana."
  },
  {
    "objectID": "intro.html#medidas-de-propagación",
    "href": "intro.html#medidas-de-propagación",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.8 Medidas de Propagación",
    "text": "1.8 Medidas de Propagación\nEn esta sección, hablaremos sobre otro conjunto de estadísticas resumidas: Medidas de propagación.\n\n1.8.1 ¿Qué es Spread?\n\nSpread es exactamente lo que parece: describe que tan separados o cerca están los puntos de datos. Al igual que las medias del centro, hay algunas medidas diferentes de extensión.\n\n\n1.8.2 La Varianza\nLa primera medida, la varianza, mide la distancia promedio desde cada punto de datos hasta la media de los datos.\n\nPara calcular la varianza, comenzamos calculando la distancia entre cada punto y la media, por lo que obtenemos un número para cada punto de datos.\n\ndists &lt;- msleep$sleep_total - mean(msleep$sleep_total)\ndists\n\n [1]  1.66626506  6.56626506  3.96626506  4.46626506 -6.43373494  3.96626506\n [7] -1.73373494 -3.43373494 -0.33373494 -7.43373494 -5.13373494 -1.03373494\n[13] -0.43373494  2.06626506 -0.13373494 -2.13373494 -1.33373494  6.96626506\n[19] -5.13373494  7.56626506 -6.53373494  9.26626506 -7.53373494 -7.33373494\n[25] -0.33373494  0.46626506  4.46626506  2.06626506 -0.63373494 -8.53373494\n[31] -7.73373494 -4.23373494 -4.13373494 -2.43373494 -0.93373494 -7.13373494\n[37]  8.96626506 -0.33373494  3.76626506  3.86626506  2.36626506  2.06626506\n[43]  9.46626506  4.16626506  0.56626506 -2.73373494  4.06626506 -2.03373494\n[49] -6.63373494 -0.73373494  5.36626506 -0.03373494  3.06626506 -1.03373494\n[55] -0.13373494  0.56626506  1.06626506  3.26626506 -6.93373494 -4.83373494\n[61]  0.66626506  7.66626506 -5.03373494  2.56626506 -1.73373494 -0.83373494\n[67] -2.03373494  0.86626506  0.16626506  6.16626506  3.36626506  5.46626506\n[73]  2.36626506 -1.33373494 -1.83373494  5.36626506 -6.03373494  5.16626506\n[79] -1.53373494 -5.23373494 -4.13373494  2.06626506 -0.63373494\n\n\nLuego elevamos al cuadrado cada distancia y luego las sumamos todas juntasd.\n\nsquared_dists &lt;- (dists)^2\nsquared_dists\n\n [1]  2.776439251 43.115836841 15.731258528 19.947523588 41.392945275\n [6] 15.731258528  3.005836841 11.790535637  0.111379010 55.260415155\n[11] 26.355234432  1.068607926  0.188125998  4.269451299  0.017885034\n[16]  4.552824793  1.778848890 48.528848890 26.355234432 57.248366962\n[21] 42.689692263 85.863668167 56.757162143 53.783668167  0.111379010\n[26]  0.217403106 19.947523588  4.269451299  0.401619974 72.824632022\n[31] 59.810656118 17.924511540 17.087764552  5.923065757  0.871860938\n[36] 50.890174191 80.393909130  0.111379010 14.184752504 14.948005516\n[41]  5.599210335  4.269451299 89.610174191 17.357764552  0.320656118\n[46]  7.473306721 16.534511540  4.136077805 44.006439251  0.538366962\n[51] 28.796800697  0.001138046  9.401981420  1.068607926  0.017885034\n[56]  0.320656118  1.136921179 10.668487444 48.076680215 23.364993468\n[61]  0.443909130 58.771619974 25.338487444  6.585716359  3.005836841\n[66]  0.695113950  4.136077805  0.750415155  0.027644070 38.022824793\n[71] 11.331740456 29.880053709  5.599210335  1.778848890  3.362583829\n[76] 28.796800697 36.405957323 26.690294673  2.352342865 27.391981420\n[81] 17.087764552  4.269451299  0.401619974\n\n\nFinalmente, dividimos la suma de las distancias al cuadrado por el número de puntos de datos menos 1, lo que nos da la varianza.\n\nsum_sq_dists &lt;- sum(squared_dists)\nsum_sq_dists\n\n[1] 1624.066\n\n\nCuanto mayor es la varianza, más dispersos están los datos. Es importante tener en cuenta que las unidades de varianza están al cuadrado, por lo que en este caso son 19.8 horas al cuadrado.\n\nsum_sq_dists/82\n\n[1] 19.80568\n\n\n\nPodemos calcular la varianza en un solo paso usando la función var de R.\n\nvar(msleep$sleep_total)\n\n[1] 19.80568"
  },
  {
    "objectID": "intro.html#desviación-estándar",
    "href": "intro.html#desviación-estándar",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.9 Desviación Estándar",
    "text": "1.9 Desviación Estándar\nLa desviación estándar es otra medida de dispersión, calculada tomando la raíz cuadrada de la varianza. En R se usa la función std para su obtención. Lo bueno de la desviación estándar es que las unidades suelen ser más faciles de entender ya que no están elevadas al cuadrado. Es mejor pensar en 4 horas y media que en 19.8 horas al cuadrado.\n\nsd(msleep$sleep_total)\n\n[1] 4.450357\n\n\n\n1.9.1 Desviación Absoluta Media\nLa desviación absoluta media toma el valor absoluto de las distancias a la media y luego toma la media de esas diferencias. Si bien esto es similar a la desviación estándar, no es exactamente lo mismo. La desviación estándar eleva al cuadrado las distancias, por lo que las distancias más largas penalizan más que las más cortas, mientras que la desviación media absoluta penaliza cada distancia por igual. Uno no es mejor que el otro, pero sd es más común que mad.\n\ndists &lt;- msleep$sleep_total - mean(msleep$sleep_total)\nmean(abs(dists))\n\n[1] 3.566701\n\n\n\n\n1.9.2 Cuartiles\nAntes de discutir la siguiente medida de dispersión, hablemos rapidamente sobre los cuartiles. Los cuartiles dividen los datos en cuatro partes iguales. Aquí llamamos a la función quantile() para obtener los cuartiles de los datos.\n\nquantile(msleep$sleep_total)\n\n   0%   25%   50%   75%  100% \n 1.90  7.85 10.10 13.75 19.90 \n\n\nEsto significa que el 25% de los datos estan entre 1.9 y 7.85, otro 25% de los datos está entre 7.85 y 10.10, y así sucesivamente. Esto significa que el segundo cuartil divide los datos en dos, con el 50% de los datos debajo y el 50% de los datos, arriba, por lo que es exactamente igual que la mediana.\n\n\n1.9.3 Gráficas de caja usan cuartiles\nLas cajas en los diagramas de cajas representan cuartiles. La parte inferior de la caja es el primer cuartil y la parte superior de la caja es el tercer cuartil. La linea media es el segundo cuartil, o la mediana.\n\nggplot(msleep, aes(y = sleep_total)) + \n  geom_boxplot()\n\n\n\n\n\n\n1.9.4 Cuantiles\nLos cuantiles, también llamados percentiles, son una versión generalizada del cuartil, por lo que pueden dividir los datos en 5 partes o diez partes, por ejemplo. De forma predeterminada la función quantile devuelve los cuartiles de los datos, pero podemos ajustar esto usando el argumento probs , que toma un vector de proporciones.\n\nquantile(msleep$sleep_total, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))\n\n   0%   20%   40%   60%   80%  100% \n 1.90  6.24  9.48 11.14 14.40 19.90 \n\n\nAquí, dividimos los datos en cinco partes iguales. También podemos usar la función seq(from, to, by) como atajo, que toma el número más bajo, el número más alto y el número por el que queremos saltar. Podemos calcular los mismos cuantiles usando seq de cero a uno, saltando de 0.2.\n\nquantile(msleep$sleep_total, probs = seq(0, 1, by = 0.2))\n\n   0%   20%   40%   60%   80%  100% \n 1.90  6.24  9.48 11.14 14.40 19.90"
  },
  {
    "objectID": "intro.html#rango-intercuartilico-iqr",
    "href": "intro.html#rango-intercuartilico-iqr",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.10 Rango Intercuartilico (IQR)",
    "text": "1.10 Rango Intercuartilico (IQR)\nEl rango intercuartilico, o IQR, es otra medida de dispersión. Es la distancia entre los percentiles 25 y 75 que también es la altura de la caja en un diagrama de caja. Podemos calcularlo usando la función cuantil para obtener 5.9 horas.\n\nquantile(msleep$sleep_total, 0.75) - quantile(msleep$sleep_total, 0.25)\n\n75% \n5.9 \n\n\n\n1.10.1 Valores atípicos\nLos valores atípicos son puntos de datos que son sustancialmente diferentes de los demás. Pero, ¿cómo sabemos qué es una diferencia sustancial? Una regla que se usa a menudo es que cualquier punto de datos menor que el primer cuartil menos 1.5 veces el IQR es un valor atípico. Así como cualquier punto mayor que el tercer cuartil más 1.5 veces el IQR.\n\n\\(data &lt; Q_1 - 1.5\\times IQR\\)\n\\(data &gt; Q_3 + 1.5\\times IQR\\)\n\n\n\n1.10.2 Encontrar valores atípicos\nPara encontrar valores atípicos, comenzaremos calculando el IQR de los pesos corporales de los mamiferos.\n\niqr &lt;- quantile(msleep$sleep_total,0.75) - quantile(msleep$sleep_total, 0.25)\n\nLuego podemos calcular los umbrales inferior y superior siguiendo las fórmulas de la diapositiva anterior.\n\nlower_threshold &lt;- quantile(msleep$sleep_total, 0.25) - 1.5*iqr\nupper_threshold &lt;- quantile(msleep$sleep_total, 0.75) + 1.5*iqr\n\nAhora podemos filtrar el marco de datos para encontrar mamiferos cuyo peso corporal esta por encima o por debajo de los umbrales.\n\nmsleep %&gt;% filter(bodywt &lt; lower_threshold | bodywt &gt; upper_threshold) %&gt;% \n  select(name, vore, sleep_total, bodywt)\n\n# A tibble: 23 × 4\n   name           vore  sleep_total bodywt\n   &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 Cheetah        carni        12.1   50  \n 2 Cow            herbi         4    600  \n 3 Goat           herbi         5.3   33.5\n 4 Asian elephant herbi         3.9 2547  \n 5 Horse          herbi         2.9  521  \n 6 Donkey         herbi         3.1  187  \n 7 Giraffe        herbi         1.9  900. \n 8 Pilot whale    carni         2.7  800  \n 9 Gray seal      carni         6.2   85  \n10 Human          omni          8     62  \n# ℹ 13 more rows\n\n\nPodemos ver que hay 23 valores atípicos de peso corporal en este conjunto de datos, incluidos la vaca y el elefante asiático."
  },
  {
    "objectID": "intro.html#ejercicio-3",
    "href": "intro.html#ejercicio-3",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.11 Ejercicio 3",
    "text": "1.11 Ejercicio 3\nLos cuantiles ya sabemos que son una excelente manera de resumir datos numéricos, ya que se pueden usar para medir el centro y la dispersión, así como para tener una idea de dónde se encuentra un punto de datos en relación con el resto del conjunto de datos. Por ejemplo, es posible que desee otorgar un descuento al 10% de los usuarios más activos en un sitio web.\nEn este ejercicio, calculará cuartiles, quantiles y deciles, que dividen un conjunto de datos en 4, 5 y 10 partes, respectivamente.\nPrimero calculemos los cuartiles de la columna co2_emssion de food_consumption.\n\nquantile(food_consumption$co2_emission)\n\n       0%       25%       50%       75%      100% \n   0.0000    5.2100   16.5300   62.5975 1712.0000 \n\n\nCalculamos ahora, los seis cuantiles que dividen los datos en 5 partes (quantiles) de la columna co2_emission de food_consumption.\n\nquantile(food_consumption$co2_emission, probs = seq(0,1,0.2))\n\n      0%      20%      40%      60%      80%     100% \n   0.000    3.540   11.026   25.590   99.978 1712.000 \n\n\nPor último, calculamos los once cuantiles de co2_emission esa división de los datos en diez partes (deciles).\n\nquantile(food_consumption$co2_emission, probs = seq(0,1,0.1))\n\n      0%      10%      20%      30%      40%      50%      60%      70% \n   0.000    0.668    3.540    7.040   11.026   16.530   25.590   44.271 \n     80%      90%     100% \n  99.978  203.629 1712.000"
  },
  {
    "objectID": "intro.html#ejercicio-4",
    "href": "intro.html#ejercicio-4",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.12 Ejercicio 4",
    "text": "1.12 Ejercicio 4\nYa vio previamente que la varianza y la desviación estándar son dos de las formas más comunes de medir la dispersión de una variable, y practicaremos su cálculo en este ejercicio. La difusión es importante ya que puede ayudar a informar las expectativas. Por ejemplo, si un vendedor vende una media de 20 productos al día, pero tiene una desviación estándar de 10 productos, probablemente habrá días en los que solo venderá uno o dos. Información como esta es importante, especialmente cuando se hacen predicciones.\nPara este ejericio, primer calcularemos la varianza y la desviación estándar de co2_emission para cada una de las food_category agrupando por y resumiendo la varianza como var_co2 y la desviación estándar como sd_co2.\n\nfood_consumption %&gt;% \n  group_by(food_category) %&gt;% \n  summarize(var_co2 = var(co2_emission),\n           sd_co2 = sd(co2_emission))\n\n# A tibble: 11 × 3\n   food_category   var_co2  sd_co2\n   &lt;fct&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n 1 beef          88748.    298.   \n 2 eggs             21.4     4.62 \n 3 fish            922.     30.4  \n 4 lamb_goat     16476.    128.   \n 5 dairy         17672.    133.   \n 6 nuts             35.6     5.97 \n 7 pork           3095.     55.6  \n 8 poultry         245.     15.7  \n 9 rice           2281.     47.8  \n10 soybeans          0.880   0.938\n11 wheat            71.0     8.43 \n\n\nLuego, creemos un histograma de co2_emission para cada una de food_category usando facet_wrap().\n\nlibrary(ggplot2)\nggplot(food_consumption, aes(co2_emission)) +\n  # Creación del  histograma\n  geom_histogram() +\n  # Creamos subgráficos separados para cada categoría de food_category\n  facet_wrap(~ food_category)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nExcelente medición de dispersión! La carne de res tiene la mayor cantidad de variación en sus emisiones de CO2, mientras que los huevos, las nueces y la soya tienen cantidades relativamente pequeñas de variación."
  },
  {
    "objectID": "intro.html#ejercicio-5",
    "href": "intro.html#ejercicio-5",
    "title": "1  Summary (resumen) Estadística",
    "section": "1.13 Ejercicio 5",
    "text": "1.13 Ejercicio 5\nLos valores atípicos pueden tener grandes efectos en las estadísticas como la media, así como en las estadísticas que se basan en la media, como la varianza y la desviación estándar. El rango intercuartilico, o IQR, es otra forma de medir la dispersión que está menos influenciada por los valores atípicos. IQR también se usa a menudo para encontrar valores atípicos. Si un valor es menor que \\(Q_1 - 1.5\\times IQR\\) o mayor que \\(Q_3 + 1.5\\times IQR\\), se considera un valor atípico. De hecho, así es como ggplot2 se calculan las longitudes de los bigotes en un diagrama de caja.\n\nEn este ejercicio, calcularemos IQR y lo utilizaremos para encontrar algunos valores atípicos.\nPrimero calculamos el total de co2_emission por país agrupando por país y tomando la suma de co2_emission. Llamaremos a la suma total_emission y almacenaremos el marco de datos resultante como emission_by_country.\n\nlibrary(tidyverse)\n\nemissions_by_country &lt;- food_consumption %&gt;%\n  group_by(country) %&gt;%\n  summarize(total_emission = sum(co2_emission))\n\nemissions_by_country %&gt;%\n  head()\n\n# A tibble: 6 × 2\n  country   total_emission\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Albania            1778.\n2 Algeria             708.\n3 Angola              413.\n4 Argentina          2172.\n5 Armenia            1110.\n6 Australia          1939.\n\n\nAhora, calcularemos el primer y tercer cuartil de total_emission y lo guardamos como q1 y q3.\n\nq1 &lt;- quantile(emissions_by_country$total_emission, 0.25)\nq3 &lt;- quantile(emissions_by_country$total_emission, 0.75)\niqr &lt;- q3-q1\n\nLuego, calculamos los puntos de corto inferior y superior para los valores atípicos de total_emission y guardamos como lower y upper.\n\nlower &lt;- q1-1.5*iqr\nupper &lt;- q3+1.5*iqr\n\nUsamos filter() para obtener países con un límite total_emission superior o inferior al límite. upper total_emission lower.\n\nemissions_by_country %&gt;%\n  filter(total_emission &lt; lower | total_emission &gt; upper)\n\n# A tibble: 1 × 2\n  country   total_emission\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Argentina          2172.\n\n\nComo puede ver, parece que Argentina tiene una cantidad sustancialmente mayor de emisiones de CO2 por persona que otros países del mundo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentos Estadísticos con R",
    "section": "",
    "text": "Introducción\nLa estadística es el estudio de cómo recopilar, anailizar y sacar conclusiones de los datos. Es una herramienta enormemente valiosa que se puede usar para enfocar para e inferir la respuesta a toneladas de preguntas. Por ejemplo ejemplo, ¿Cuál es la probabilidad de que alguien compre su producto, cuántas llamadas recibirá su equipo de soporte y cuántas tallas de jeans debe fabricar para adaptarse al 95% de la población? En este curso, usará datos de ventas para descubrir cómo responder preguntas como estas a medida que que aumenta sus habilidades estadísticas y aprende a calcular promedios, usar diagramas de dispersión para mostrar la relación entre valores numéricos y calcular la correlación. También abordará la probabilidad, la columna vertebral del razonamiento estadístico, y aprenderá a realizar un estudio bien deseñado para sacar sus propias conclusiones a partir de los datos."
  },
  {
    "objectID": "Teorema_LC.html#la-distribución-normal",
    "href": "Teorema_LC.html#la-distribución-normal",
    "title": "3  Más Distribuciones y el Teorema del Límite Central",
    "section": "3.1 La distribución Normal",
    "text": "3.1 La distribución Normal\nLa siguiente distribución de probabilidad que discutiremos es la distribución normal. Es una de las distribuciones de probabilidad más importantes que aprenderá, ya que innumerables métodos estadísticos se basan en ella y se aplica a más situaciones del mundo real que las distribuciones que hemos cubierto hasta ahora.\n\n3.1.0.1 ¿Qué es la distribución normal?\nLa distribución normal se ve así.\n\nSu forma se conoce comúnmente como curva de campana. La distribución normal tiene algunas propiedades importantes.\n\n\n3.1.0.2 Simétrica\nPrimero, es simétrica, por lo que el lado izquierdo es una imagen especular del lado derecho.\n\n\n\n3.1.0.3 Área = 1\nSegundo, como cualquier distribución continua, el área bajo la curva es 1.\n\n\n\n3.1.0.4 La curva nunca llega a 0\nEn tercer lugar, la probabilidad nunca llega a 0, incluso si lo parece en los extremos de la cola. Solo el 0.006% de su área está contenida más alla de los bordes de este gráfico.\n\n\n\n3.1.1 Descrita por media y desviación estándar\nLa distribución normal se describe por su media y desviación estándar. Aquí hay una distribución normal con una media de 20 y una desviación estándar de 3\n\nAquí hay una distribución normal con una media de 0 y una desviación estandar de 1.\n\nCuando una distribución normal tiene una media de 0 y una desviación estándar de 1, es una distribución especial llamada distribución normal estándar. Observe cómo ambas distribuciones tienen la misma forma\n\npero sus hachas tienen diferentes escalas.\n\n\n3.1.2 Áreas bajo la distribución normal\nPara la distribución normal, el 68% del área está dentro de 1 desviación estándar de la media.\n\n95% del área cae dentro de 2 desviaciones estándar de la media\n\nY dl 99.7% del área cae dentro de tres desviaciones estándar. Esto a veces se llama la regla del 68%-95%-99.7%.\n\n\n\n3.1.3 Muchos histogramas parecen normales\nHay muchos datos del mundo real en forma de distribución normal. Por ejemplo, aquí hay un histograma de las alturas de las mujeres que participaron en la Encuesta Nacional de Examen de Salud y Nutrición.\n\nnote que la altura media es de alrededor de 161 centímetros y la desviación estándar es de unos 7 centimetros.\n\n\n3.1.4 Aproximación de datos con la distribución normal\nDado que estos datos de altura se asemejan mucho a la distribución normal, podemos tomar el área bajo una distribución normal con media 161 y desviación estándar 7 para aproximar qué porcentaje de mujeres se encuentra en diferentes rangos de altura.\n\n\n3.1.4.1 ¿Qué porcentaje de mujeres mide menos de 154 cm?\n\nPodemos responder esto usando la función pnorm, que toma el área de la distribución normal menor que algún número. Pasamos el número de interés, 154, así como la media y la desviación estándar de la distribución normal que estamos usando. Esto nos da que alrededor del 16% de las mujeres miden menos de 154 centímetros.\n\npnorm(154, mean = 161, sd = 7)\n\n[1] 0.1586553\n\n\n\n\n3.1.4.2 ¿Qué porcentaje de mujeres mide más de 154 cm?\n\nPara encontrar el porcentaje de mujeres que miden más de 154 centímetros, podemos agregar el cuarto argumento lower.tail = FALSE, lo que tomará el área a la derecha del primer argumento.\n\npnorm(154, mean = 161, sd = 7, lower.tail = FALSE)\n\n[1] 0.8413447\n\n\n\n\n\n3.1.5 ¿Qué porcentaje de mujeres mide 154-157 cm?\n\nPara obtener el porcentaje de mujeres entre 154 y 157 centímetros de altura, podemos tomar el área por debajo de 157 y restar el área por debajo de 154, lo que nos deja el área entre 154 y 157.\n\npnorm(157, mean = 161, sd = 7) - pnorm(154, mean = 161, sd = 7)\n\n[1] 0.1251993\n\n\n\n\n3.1.6 ¿Qué estatura es más baja que el 90% de las mujeres?\n\nTambién podemos calcular porcentajes a partir de alturas usando qnorm(). Para averiguar que estatura es más baja que el 90% de las mujeres, pasamos 0.9 en qnorm junto con las misma media y desviación estándar con las que hemos estado trabajando.\n\nqnorm(0.9, mean = 161, sd = 7)\n\n[1] 169.9709\n\n\nEsto nos dice que el 90% de las mujeres miden menos de 170 centímetros.\n\n\n3.1.7 ¿Qué altura es el 90% de las mujeres más altas que?\n\nDel mismo modo, podemos calcular la altura que el 90% de las mujeres son más altas que al establecer el argumento de cola de punto inferior de qnorm en FALSO.\n\nqnorm(0.9, mean = 161, sd = 7, lower.tail = FALSE)\n\n[1] 152.0291\n\n\n\n\n3.1.8 Generaciones de números aleatorios\nAl igual que con otras distribuciones, podemos generar números aleatorios a partir de una distribución normal usando rnorm(), pasando el tamaño de muestra que queremos junto con la media y la desviación estándar de la distribución. Aquí, hemos generado 10 alturas aletorias más.\n\nrnorm(10, mean = 161, sd = 7)\n\n [1] 165.5898 168.9200 158.4965 154.4749 171.6224 168.9358 155.4495 164.5532\n [9] 162.2216 163.1967\n\n\nAquí hemos generado 10 alturas aleatorias más.\n\n\n3.1.9 Ejercicio 14\nVolvamos al ejercicio de los tratos cerrados por parte de Amir, dado que cada trato en el trabajó de Amir (tanto ganado como perdido) era diferente, cada uno valía una cantidad diferente de dinero. Estos valores siguien una distribución normal con una meida de 5000 dólares y una desviación estándar de 2000 dólares. Como parte de sus métricas de desempeño, desea calcular la probabilidad de que Amir cierre un trato por valor de varias cantidades.\n\n¿Cuál es la probabilidad de que Amir cierre un trato por menos de $7,500?\n\\[\nP(trato &lt; 7500)\n\\]\n\npnorm(7500, mean = 5000, sd = 2000)\n\n[1] 0.8943502\n\n\n¿cuál es la probabilidad de que Amir cierre un trato por valor de más de $1000?\n\\[\nP(trato &gt; 1000)\n\\]\n\n1 - pnorm(1000, mean = 5000, sd = 2000)\n\n[1] 0.9772499\n\n\nExplicación: ¿por qué hago 1 - p? Pues la función de pnorm busca las probabilidades más pequeñas, es decir, me buscara la probabilidad que Amir cierre un trato que valga menos de $1000. Entonces todo lo que no sea menor que 1000 será superior, por lo que le resto la probabilidad\nLa probabilidad que cierre uno menor que $1000 es:\n\npnorm(1000, mean = 5000, sd = 2000)\n\n[1] 0.02275013\n\n\n¿Cuál es la probabilidad de que Amir cierre un trato por valor de entre $3000 y $7000?\n\npnorm(7000, mean = 5000, sd = 2000) - pnorm(3000, mean = 5000, sd = 2000)\n\n[1] 0.6826895\n\n\n¿Qué cantidad será más del 75% de las ventas de amir?\n\nqnorm(0.75, mean  =5000, sd = 2000, lower.tail = FALSE)\n\n[1] 3651.02\n\n\nExplicación: en la teória he mencionado que habia 2 fórmulas. Esta es la segunda, aquí vamos a buscar los valores que pueden llegar a ganar con la probabilidad. Bien, esta fórmula, como la anterior, busca la probabilidad hacia la izquierda, es decir, me buscará la cantidad de ventas en las que la cantidad sea menor al 75% de las ventas:\n\nqnorm(0.75, mean = 5000, sd = 2000)\n\n[1] 6348.98\n\n\nSin embargo, no es lo que buscamos, nos preguntan que podemos hacer en el 75% del tiempo de Amir, en sus ventas, pues, cerrara tratos por encima de los $3,651.02.\n\n\n\n3.1.10 Ejercicio 15\nSimulación de ventas bajo nuevas condiciones de mercado.\nEl analista financiero de la empresa predice que el próximo trimestre, el valor de cada venta aumentará un 20% y la volatidad, o desviación estándar, del valor de cada venta1 aumentará un 30%. Para ver cómo podrían ser la ventas de Amir el próximo trimestre en estas nuevas condiciones de mercado, simulará nuevos montos de ventas utilizando la distribución normal y los almacenará en una variable new_sales, que crearemos ahora mismo en R:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nnew_sales &lt;- seq(1, 36, 1) %&gt;% \n  as.data.frame() %&gt;% \n  set_names(\"sale_num\")\n\n\nActualmente, el monto de venta promedio de Amir es de $5000. Calcule cuál será su nueva cantidad promedio si aumenta en un 20% y guárdela en new_mean.\n\nnew_mean &lt;- 5000*1.2\nnew_mean\n\n[1] 6000\n\n\nLa desviación estándar actual de amir es $2000. Calcule cuál será su nueva desviación estándar si aumenta en un 30% y guárdela en new_sd.\n\nnew_sd &lt;- 2000 * 1.3\nnew_sd\n\n[1] 2600\n\n\nAgregue una nueva columna llamada amount al marco de datos new_sales, que contiene 35 cantidades simuladas de una distribución normal con media de new_mean y una desviación estándar de new_sd.\n\nnew_sales &lt;- new_sales %&gt;% \n  mutate(amount = rnorm(36, mean = new_mean, sd = new_sd))\n\nTrace la distribución de new_sales y amount usando un histograma con 10 contenedores.\n\nggplot(new_sales, aes(amount)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n3.1.11 Ejercicio 16\n¿Qué mercado es mejor?\nLa métrica clave que utiliza la empresa para evaluar a los vendedores es el porcentaje de ventas que realizan por encima de $1000, ya que el tiempo dedicado a cada venta suele valer un poco más que eso, por lo que cuanto mayor sea esta métrica, mejor será el rendimiento del vendedor.\nRecuerde que las cantidades de ventas actuales de amir tienen una media de $5000 y una desviación estándar de $2000, y las cantidades pronosticadas de Amir en el mercado del proximo trimestre tienen una media de $6000 y una desviación estándar de $2600.\nBasado solo en la métrica del porcentaje de ventas de más de $1000, ¿Amir se desempeña mejor en el mercado actual o en el mercado previo?\nRespuesta: Amir se desempeña casi por igual en ambos mercados."
  },
  {
    "objectID": "Teorema_LC.html#teorema-del-límite-central",
    "href": "Teorema_LC.html#teorema-del-límite-central",
    "title": "3  Más Distribuciones y el Teorema del Límite Central",
    "section": "3.2 Teorema del Límite Central",
    "text": "3.2 Teorema del Límite Central\nAhora que está familiarizado con la distribución normal, es hora de aprender qué es lo que la hace tan importante.\n\n3.2.1 Tirar los dados 5 veces\nvolvamos a nuestro ejemplo de lanzamiento de dados. Tenemos un vector de los números del 1 al 6 llamado die.\n\ndie &lt;- c(1,2,3,4,5,6)\n\nPara simular tirar el dado 5 veces, usaremos la función sample() . Sample() funciona de la misma manera que sample_n(), excepto que toma muestras de un vector en lugar de un marco de datos. Pasamos el vector del que queremos muestrear, el tamaño de la muestra, y establecemos replace en TRUE. Esto nos da los resultados de 5 casos.\n\nsample_of_5 &lt;- sample(die, 5, replace = TRUE)\nsample_of_5\n\n[1] 6 1 6 3 3\n\n\nAhora, tomaremos la media de los 5 casos, lo que nos da 3.6.\n\nmean(sample_of_5)\n\n[1] 3.8\n\n\n\nsample(die, 5, replace = TRUE) %&gt;% mean()\n\n[1] 3.8\n\n\nSi lo hacemos de nuevo obtenemos otra media\n\nsample(die, 5, replace = TRUE) %&gt;% mean()\n\n[1] 3.2\n\n\nRepitamos esto 10 veces: tiraremos 5 veces y sacaremos la media. Para hacer esto, usaremos replicate(). Le pasamos 10 para que el proceso se repita 10 veces, seguido del fragmento de código que queremos que se ejecute, que es el balanceo y el promedio. Esto devuelve un vector de 10 medias de muestra diferentes.\n\nsample_mean &lt;- replicate(10, sample(die, 5, replace = TRUE) %&gt;% mean())\nsample_mean\n\n [1] 3.8 2.2 3.8 4.0 3.0 2.2 4.0 3.8 3.8 3.8\n\n\n\n\n3.2.2 Distribuciones Muestrales\n\nUna distribución de una estadística de resumen como esta se denomina distribución de muestreo. Esta distribución, especificamente, es una distribución muestral de la media muestral.\n\n3.2.2.1 Medias de 100 muestras\nAhora hagamos esto 100 veces. Si observamos la nueva distribución de muestreo, su forma se parece un a la distribución normal, aunque la distribución del dado es uniforme, auqnue la distribución del dado es uniforme.\n\nreplicate(100, sample(die, 5, replace = TRUE) %&gt;% mean())\n\n  [1] 2.0 4.6 2.8 3.8 3.4 4.2 2.2 4.8 3.2 3.4 3.6 4.6 4.6 2.8 3.4 3.8 3.8 4.0\n [19] 3.2 3.6 3.4 3.4 3.0 2.0 4.0 3.2 2.6 4.6 3.0 3.8 3.6 3.6 3.6 4.6 3.8 4.0\n [37] 3.8 3.8 4.0 4.4 3.0 5.0 2.8 3.2 2.4 3.6 4.4 3.6 3.4 4.6 3.6 2.6 2.4 2.6\n [55] 3.4 3.0 4.0 2.6 3.6 3.6 1.4 4.4 4.8 2.8 3.4 4.0 3.0 2.6 3.8 1.2 3.6 3.4\n [73] 3.4 2.2 2.2 3.2 3.8 3.2 4.6 4.0 3.6 3.0 4.0 3.0 3.6 2.6 2.2 4.0 3.6 3.2\n [91] 4.8 1.4 3.2 3.2 2.2 3.4 3.2 2.8 2.0 4.8\n\n\n\n\n\n3.2.2.2 1000 medias de muestra\nAhora tomemos 1000 medias.\n\nsample_means &lt;- replicate(1000, sample(die, 5, replace = TRUE) %&gt;% mean())\n\n\nEsta distribución de muestreo se parece más a la distribución normal.\nEste fenomeno se conoce como teorema del límite central.\n\n\n\n3.2.3 Teorema del Límite Central\n\nEstablece que una distribución muestral se acercará a una distribución normal a medida que aumenta el número de ensayos. En nuestros ejemplo, la distribución muestral se acercó más a la distribución normal a medida que tomamos más y más medias muestrales. Es importante tener en cuenta que el teorema del límite central solo se aplica cuando las muestras se toman al azar y son independientes, por ejemplo, al seleccionar acuerdos de venta con reemplazo.\n\n3.2.3.1 Desviación Estándar y TLC\nEl teorema del límite central, o TLC, también se aplica a otras estadísticas de resumen. Si tomamos la desviación estándar de 5 intentos 1000 veces, las desviaciones estándar de la muestra se distribuyen normalmente, centrados alrededor de 1.9, que es la desviación estándar de la distribución.\n\nsample_sd &lt;- replicate(1000, sample(die, 5, replace = TRUE) %&gt;% sd())\n\n\n\n\n3.2.3.2 Proporciones y el TLC\nOtra estadística a la que se aplica la condición del TLC es a la proporción.\nHagamos una muestra del equipo de ventas 10 veces con reemplazo y veamos cuántos sorteos tienen a Claire como resultado.\n\nsales_team &lt;- c(\"Amir\", \"Brian\", \"Claire\", \"Damian\")\nsample(sales_team, 10, replace = TRUE)\n\n [1] \"Claire\" \"Damian\" \"Claire\" \"Claire\" \"Amir\"   \"Amir\"   \"Claire\" \"Claire\"\n [9] \"Claire\" \"Claire\"\n\n\nEn este caso el 20% de los sorteos fueron de Claire. Si volvemos a sortear\n\nsample(sales_team, 10, replace = TRUE)\n\n [1] \"Amir\"   \"Amir\"   \"Amir\"   \"Brian\"  \"Damian\" \"Damian\" \"Claire\" \"Amir\"  \n [9] \"Amir\"   \"Claire\"\n\n\nHay un 40% de Claire.\n\n\n\n3.2.4 Distribución muestral de proporciones\nSi repetimos esto 1000 veces y gráficamos la distribución de las proporciones de la muestra, parece una distribución normal centrada al rededor de 0.25, ya que el nombre de Claire estaba en el 25% de los boletos.\n\n\n\n3.2.5 Media de la distribución muestral\nDado que estas distribuciones de muestreo son normales, podemos tomar su media para obtener una estimación de la media, la desviación estándar o la proporción de una distribución. Si tomamos la media de nuestras medias muestrales anteriores, obtenemos\n\nmean(sample_means)\n\n[1] 3.461\n\n\nEsto está bastante cerca del valor esperado, que es 3.5. De manera similar, la media de las proporciones muestrales de Claires no está muy lejos de 0.25. En estos ejemplos, sabemos cómo son las distribuciones subyacentes, pero si no lo sabemos, esto puede ser un método útil para estimar las características de una distribución subyacente.\nEl teorema del límite central también es útil cuando tienes una gran población y no tienes el tiempo o los recursos para recopilar datos sobre todos. En su lugar, puede recolectar varias muestras más pequenas y crear una distribución de muestreo para estimar cuál es la media o la desviación estándar.\n\n\n3.2.6 Distribución de Poisson\nHablemos sobre otra distribución de probabilidad llamada distribución de Poisson.\n\n3.2.6.1 Proceso de Poisson\nAntes de hablar de probabilidad, definamos un proceso de Poisson. Un proceso de Poisson es un proceso en el que los eventos parecen suceder a una determinada velocidad, pero completamente al azar. Por ejemplo, el número de animales adoptados de un refugio de animales cada semana es un proceso de Poisson: Podemos saber que en promedio hay 8 adopciones por semana, pero este número puede diferir aleatoriamente. Otros ejemplos serían la cantidad de personas que llegan a un restaurante cada hora, o la cantidad de terremotos por año en california.\n\n\n3.2.6.2 Distribución de Poisson\nLa distribución de Poisson describe la probabilidad de que ocurra una cierta cantidad de eventos durante un período fijo de tiempo. Podemos usar la distribución de Poisson para calcular la probabilidad de que al menos \\((\\geq)\\) 5 animales sean adoptados en una semana, la probabilidad de que 12 personas lleguen a un restaurante en una hora o la probabilidad de menos \\((&lt;)\\) de 20 terremotos en California en un año.\n\n\n3.2.6.3 Lambda \\((\\lambda)\\)\nLa distribución de Poisson se describe mediante un valor denominado lambda, que representa el número promedio de eventos por período de tiempo. El ejemplo del refujio de animales, este sería el número promedio de adopciones por semana, que es 8. Este valor también es el valor esperado de la distribución. La distribución de Poisson con \\(\\lambda = 8\\) se ve así.\n\nNote que es una distribución discreta ya que estamos contando eventos 7 y 8 son el número más probable de adopciones que se producirán en una semana.\n\n\n3.2.6.4 Lambda es el pico de la distribución\nLambda cambia la forma de la distribución, por lo que una distribución de Poisson con lambda igual a 1, curva color rojo, se ve muy diferente a una distribución Poisson con lambda igual a 8, en azul, pero pase lo que pase, el pico de la distribución siempre estpa en su valor lambda.\n\n\n\n3.2.6.5 Probabilidad de un solo valor\n\\[\nP(\\mbox{\\# de adopciones en una semana} = 5)?\n\\]\nDado que el número promedio de adopciones por semana es 8, ¿cuál es la probabilidad de 5 adopciones en una semana? Usaremos la función dpois, pasando 5 como primer argumento y 8 como segundo argumento para indicar la media de la distribución. Esto nos da alrededor del 9%.\n\ndpois(5, lambda = 8)\n\n[1] 0.09160366\n\n\n\n\n3.2.6.6 Probabilidad de menor o igual que\n\\[\nP(\\mbox{\\# de adopciones en una semana}\\leq 5)?\n\\]\nPara obtener la probabilidad de que ocurran 5 adopciones o menos en una semana, usamos la función ppois(), pasando los mismos números. Esto nos da alrededor del 20%\n\nppois(5, lambda = 8)\n\n[1] 0.1912361\n\n\n\n\n3.2.6.7 Probabilidad de mayor que\n\\[\nP(\\mbox{\\# de adopciones en una semana} &gt; 5)?\n\\]\nAl igual que otras funciones de probabilidad que ha aprendido hasta ahora, use el argumento de cola de punto inferior para obtener la probabilidad de más de 5 adopciones.\n\nppois(5, lambda = 8, lower.tail = FALSE)\n\n[1] 0.8087639\n\n\nHay un 81% de posibilidades de que se produzcan más de 5 adopciones. Si el número promedio de adopciones aumenta a 10 por semana,\n\nppois(5, lambda = 10, lower.tail = FALSE)\n\n[1] 0.932914\n\n\nhabrá un 93% de posibilidades de que ocurra más de 5 adopciones.\n\n\n\n3.2.7 Muestreo a Partir de una distribución de Poisson\nAl igual que otras distribuciones, podemos tomar muestras de las distribuciones de Poisson usando rpois(). Aquí, simularemos 10 semanas diferentes en el refugio de animales.\n\nrpois(10, lambda = 8)\n\n [1]  5  6  7  6  7 13  6  7  9  6\n\n\nEn una semana hay 6 adopciones, pero en otra solo 5.\n\n3.2.7.1 EL Torema del Límite Central aún se Aplica\nAl igual que otras distribuciones, la distribución muestral de medias muestrales de una distribución de Poisson parece normal con una gran cantidad de muestras.\n\n\n\n\n3.2.8 Ejercicio 17\nAhora que ha aprendido acerca de la distribución de Poisson, sabe que su forma está descrita por un valor llamado lambda. En este ejercicio, relacionará histogramas con valores lambda.\n\n\n\n3.2.9 Ejercicio 18\nSu empresa utiliza software de ventas para realizar un seguimiento de los nuevos clientes potenciales de ventas. Los organiza en una cola para que cualquiera pueda hacer un seguimiento de uno cuando tenga un poco de tiempo libre. Dado que el número de respuestas de clientes potenciales es un resultado contable durante un período de tiempo, este escenario corresponde a una distribución de Poisson. En promedio, Amir responde a 4 clientes potenciales cada día. En este ejercicio, calculará las probabilidades de que Amir responda a diferentes cantidades de clientes potenciales.\n\n¿Cuál es la probabilidad de que Amir responda a 5 clientes potenciales en un dia, dado que responde a un promedio de 4 clientes?\n\ndpois(5, lambda = 4)\n\n[1] 0.1562935\n\n\nEl compañero de trabajo de Amir respnde a un promedio de 5.5 clientes potenciales por día. ¿Cuál es la probabilidad de que responda 5 clientes en un día?\n\ndpois(5, 5.5)\n\n[1] 0.1714007\n\n\n¿Cuàl es la probabilidad de que Amir responda a 2 clientes potenciales o menos en un día?\n\nppois(2, lambda = 4)\n\n[1] 0.2381033\n\n\n¿Cuál es la probabilidad de que Amir responda a más de 10 clientes potenciales en un día?\n\nppois(10, lambda = 4, lower.tail = FALSE)\n\n[1] 0.002839766\n\n\n\nObservación: Tenga en cuenta que si proporciona dpois() o ppois() con un número no entero, devuelve 0 y muestra una advertencia, ya que la distribución de Poisson solo se aplica a los números enteros.\n\n\n3.2.10 Mas distribuciones de Probabilidad\nAquí discutiremos algunas otras distribuciones de probabildad.\n\n3.2.10.1 Distribución Exponencial\nLa primera distribución es la distribución exponencial, que representa la probabilidad de que pase cierto tiempo entre eventos de Poisson. Podemos usar la distribución exponencial, para predecir, por ejemplo, la probabilidad de más de 1 día de adopciones, la probabilidad de menos de 10 minutos entre llegadas a restaurantes y la probabilidad que pasen de 6 a 8 meses entre terremotos. La distribución exponencial utiliza el mismo valor \\(\\lambda\\), que representa la tasa, que la distribución de Poisson. Tenga en cuenta que lambda y tasa significan el mismo valor en este contexto. También es continua, a diferencia de la distribución de Poisson, ya que representa el tiempo.\n\n3.2.10.1.1 Solicitudes de atención al cliente\nPor ejemplo, digamos que se crea un ticket de sevicio al cliente cada 2 minutos. Podemos reformular esto para que sea en términos de un intervalo de tiempo de un minuto, por lo que se crea la mitad de un ticket cada minuto. Usaremos 0.5 como valor lambda. La distribución exponencial con una tasa de un medio se ve así.\n\n\n\n\n3.2.10.2 Lambda en Distribución Exponencial\nLa tasa afecta la forma de la distribución y qué tan abruptamente declina.\n\n\n\n\n3.2.11 Probabilidades\nSimilar a otras distribuciones continuas, podemos usar pexp() para calcular probabilidades.\nSe calcula la probabilidad de esperar menos de 1 minuto por una nueva solicitud.\n\\[\nP(esperar &lt; \\mbox{1 minuto})\n\\]\n\npexp(1, rate = 0.5)\n\n[1] 0.3934693\n\n\nlo que nos da un 40% de posibilidades.\nLa probabilidad de esperar más de 4 minutos\n\\[\nP(esperar &gt; 4min)\n\\]\nse puede encontrar utilizando la cola de punto inferior igual a FALSO, lo que da una probabilidad del 13%.\n\npexp(4, rate = 0.5, lower.tail = FALSE)\n\n[1] 0.1353353\n\n\nFinalmente la probabilidad de esperar entre 1 y 4 minutos.\n\\[\nP(1min &lt; esperar &lt; 4 min)\n\\]\n\npexp(4, rate = 0.5) - pexp(1, rate = 0.5)\n\n[1] 0.4711954\n\n\nPor tanto, hay un 50% de posibilidades de esperar entre 1 y 4 minutos.\n\n\n3.2.12 Valor esperado de la Distribución Exponencial\nRecuerde que lambda es el valor esperado de la distribución de Poisson, que mide la frecuencia en términos de tasa o número de eventos.\nEn nuestro ejemplo de ticket de servicio al cliente, esto significa que la cantidad esperada de solicitudes por minuto es 0.5.\nLa distribución exponencial mide la frecuencia en términos de tiempo entre eventos. El valor esperado de la distribución exponencial se puede calcular tomando 1 dividido por lambda.\nEn nuestro ejemplo, el tiempo esperado entre solicitudes es \\[\\frac{1}{0.5} = 2\\]\nPor lo que hay un promedio de 2 minutos entre solicitudes.\n\n\n3.2.13 Distribución t-Students\n\nLa siguiente distribución es la distribución t, que aveces también se denomina distribución t de Student.\nSu forma es similar a la distribución normal, pero no exactamente igual.\nSin comparamos la distribución normal, en rojo, con la distribución t, en verde, con 1 grado de libertad, las colas de la distribución t son más gruesas. Esto significa que en una distribución t, es más probable que las observaciones se alejen más de la media.\n\n3.2.13.1 Grados de Libertad\n\nLa distribución t tiene un parámetro llamado grados de libertad, que afecta el grosor de las colas de la distribución.\nLos grados de libertad más bajos dan como resultado colas más gruesas y una desviación estándar más alta.\nA medida que aumenta el número de grados de libertad, la distribución se parece cada vez más a la distribución normal.\n\n\n\n3.2.14 Distribución Log-Normal\nLa última distribución que discutiremos es la distribución log-normal.\nLas variables que siguen una distribución logarítmica normal tienen un logarítmo que se distribuye normalmente. Esto da como resultado distribuciones asimétricas, a diferencia de la distribución normal.\n\nExisten muchos ejemplos del mundo real que siguen esta distribución, como la longitud de juegos de ajedrez, la presión arterial en adultos y el número de hospitalizaciones en el brote SARS de 2003.\nAdemás de las tres distribuciones que vimos en esta sección, hay muchas otras distribuciones de probabilidad.\n\n\n3.2.15 Ejercicio 19\nEn este punto, ha aprendido acerca de tantas distribuciones de probabilidad diferentes que pueden ser difícil recordar cuál es cuál. En este ejercicio, practicaremos distinguir entre distribuciones e identificar la distrubución que mejor se adapte a diferentes escenarios\nEn la siguiente tabla relacionamos cada situación con la distribución que mejor la modele.\n\n\n\n\n\n\n\n\nPoisson\nExponencial\nBinomio\n\n\n\n\nNúmero de clientes que entran en una tienda cada hora.\nCantidad de tiempo hasta que alguien paga su préstamo.\nNúmero de personas de un grupo de 30 que aprueban su examen de conducir.\n\n\nNúmero de productos vendidos cada semana.\nCantidad de tiempo hasta que el próximo cliente haga su compra.\n\n\n\n\n\n\n3.2.16 Ejercicio 20\nPara evaluar más a fondo el rendimiento de Amir, querrá saber cuánto tiempo le lleva responder a un cliente potencial después de abrirlo. En promedio, tarde 2.5 horas en responder. En este ejercicio, calculará las probabilidades de las diferentes cantidades de tiempo que transcurren entre que Amir recibe un cliente potencial y envía una respuesta.\n\n¿Cuál es la probabilidad de que Amir tarde menos de una hora en responder a un cliente potencial?\n\npexp(1, rate = 1/2.5)\n\n[1] 0.32968\n\n\n¿Cuál es la probabilidad de que Amir tarde más de 4 horas en responder a un cliente potencial?\n\npexp(4, rate = 1/2.5, lower.tail = FALSE)\n\n[1] 0.2018965\n\n\n¿Cuál es la probabilidad de que Amir tarde entre 3 y 4 horas en responder a un cliente potencial?\n\npexp(4, rate = 1/2.5) - pexp(3, rate = 1/2.5)\n\n[1] 0.09929769"
  },
  {
    "objectID": "correlacion_de.html",
    "href": "correlacion_de.html",
    "title": "4  Correlación y Diseño Experimental",
    "section": "",
    "text": "En este capítulo, aprenderá a cuantificar la fuerza de una relación lineal entre dos variables y explorará cómo las variables de confusión pueden afectar la relación entre otras dos variables. También verá cómo el diseño de un estudio puede influir en sus resultados, cambiar la forma en que se deben analizar los datos y afectar potencialmente la confiabilidad de sus conclusiones."
  },
  {
    "objectID": "correlacion_de.html#correlación",
    "href": "correlacion_de.html#correlación",
    "title": "4  Correlación y Diseño Experimental",
    "section": "4.1 Correlación",
    "text": "4.1 Correlación\n\nHablemos de las relaciones entre las variables numéricas. Podemos visualizar este tipo de relaciones con diagramas de dispersión: en este diagrama de dispersión, podemos ver la relación entre la cantidad total de sueño que obtienen los mamíferos y la cantidad de sueño REM que obtienen.\n\nla variable en el eje x se llama explicativa o independiente\ny la variable en el eje y se llama respuesta o variable dependiente.\n\n\n4.1.1 Coeficiente de correlación\nTambién podemos examinar las relaciones entre dos variables numéricas usando un número llamado coeficiente de correlación. Este es un número entre -1 y 1, donde la magnitud corresponde a la fuerza de la relación entre las variables, y el signo, positivo o negativo, corresponde a la dirección de la relación.\n\n\n4.1.2 Magnitud = fuerza de relación\n\nAquí hay un diagrama de dispersión de 2 variables, x e y, que tienen un coeficiente de correlación de 0.99. Dado que los puntos de datos están muy agrupados alrededor de la línea, podemos describir esto como una relación casi perfecta o muy fuerte. Si sabemos que es x, tendremos una idea bastante fuerte de cuál podría ser el valor de y.\nEn la siguiente figura x e y tienen un coeficiente de correlación de 0.75, y los puntos de los datos están más dispersos.\n\nEn este otro gráfico, x e y tiene una correlación de 0.56 y, por lo tanto, están moderadamente correlacionados.\n\nUn coeficiente de correlación alrededor de 0.2 se consideraría una relación débil.\n\nCuando el coeficiente de correlación es cercano a 0, x e y no tienen relación y el diagrama de dispersión parece completamente aleatorio.\n\nEsto significa que conocer el valor de x no nos dice nada sobre el valor de y.\n\n\n4.1.3 Signo = Dirección\nEl signo de coeficiente de correlación corresponde a la dirección de la relación. Un coeficiente de correlación positivo indica que a medida que \\(x\\) aumenta, \\(y\\) tambien aumenta. Un coeficiente de correlación negativo indica que a medida que \\(x\\) aumenta, \\(y\\) disminuye.\n\n\n\n4.1.4 Visualizar relaciones en R\nPara visualizar relaciones entre dos variables, podemos usar un diagrama de dispersión creado usando geom_point. Tal como se sigue.\n\ndf &lt;- data.frame(x = c(runif(20, min = 1, max = 15)), y = c(runif(20, min = 1, max = 15)))\n\n\nlibrary(ggplot2)\nggplot(df, aes(x, y)) + \n  geom_point()\n\n\n\n\n\n\n4.1.5 Agregar una línea de tendencia\nPodemos agregar una línea de tendencia lineal al diagrama de dispersión usando geom_smooth del método en lm para indicar que queremos una linea de tendencia lineal y lo cambiaremos a FALSE para que no haya una margenes de error alrededor de la línea. Las líneas de tendencia como esta pueden ser útiles para ver más facilmente una relación entre dos variables.\n\nggplot(df, aes(x,y)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n4.1.6 Computo de Correlación en R\nPara calcular el coeficiente de correlación entre dos variables aleatorias en R, podemos utilizar la función cor(). La función cor() toma dos vectores numéricos y devolverá su coeficiente de correlación. Tenga en cuenta que no importa en qué orden se pasan los vectores a la función, ya que la correlación entre \\(x\\) e \\(y\\) es lo mismo que la correlación entre \\(y\\) e \\(x\\).\n\ncor(df$x, df$y)\n\n[1] -0.0007910694\n\n\n\ncor(df$y, df$x)\n\n[1] -0.0007910694\n\n\n\n\n4.1.7 Correlación con Valores NA\nSi tiene valores faltantes en cualquiera de las variables, R devolverá NA como resultado de la correlación. Para ignorar los puntos de datos en los que faltan ambos valores, establezca el argumento use = \"pairwise.complete.obs\".\n\n\n4.1.8 Muchas formas de calcular la correlación\nHay más de una forma de calcular la correlación, pero el método que hemos estado usando en esta sección se llama correlación producto momento de Pearson, que también se escribe como (\\(r\\)).\n\nEsta es la medida de correlación más utilizada\nMatemáticamente, se calcula utilizando esta fórmula\n\\[r = \\sum_{i=1}^n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{\\sigma_x \\times \\sigma_y}\\] donde \\(\\bar{x}\\) y \\(\\bar{y}\\) son las medias de \\(x\\) e \\(y\\), y \\(\\sigma_x\\), \\(\\sigma_y\\) son las desviaciones estándar de \\(x\\) e \\(y\\).\nNo es importante memorizar la fórmula en sí, pero sepa que hay variaciones de esta fórmula que miden la correlación un poco diferente, como la tau de Kendall y la rho de Spearman, pero están fuera del alcance de este curso.\n\n\n\n4.1.9 Ejercicio 20\nEn este capítulo trabajaremos con un conjunto de datos world_happiness que contiene los resultados del informe mundial de la felicidad de 2019. El informe califica a varios países en función de cuán felices son las personas en ese país.\nTambién clasifica a cada país en varios aspectos sociales, como el apoyo social, la libertad, la corrupción y otros. El conjunto de datos también incluye el PIB per cápita y la esperanza de vida de cada país.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nworld_happiness &lt;- readRDS(\"world_happiness_sugar.rds\")\nhead(world_happiness)\n\n      country social_support freedom corruption generosity gdp_per_cap life_exp\n1     Finland              2       5          4         47       42400     81.8\n2     Denmark              4       6          3         22       48300     81.0\n3      Norway              3       3          8         11       66300     82.6\n4     Iceland              1       7         45          3       47900     83.0\n5 Netherlands             15      19         12          7       50500     81.8\n6 Switzerland             13      11          7         16       59000     84.3\n  happiness_score grams_sugar_per_day\n1             155                86.8\n2             154               152.0\n3             153               120.0\n4             152               132.0\n5             151               122.0\n6             150               166.0\n\n\nEn este ejercicio, examinará la relación entre la esperanza de vida (life_exp) y la puntuación de felicidad (happiness_score) de un país, tanto visual como cuantitativamente.\n\nPrimero, creemos un diagrama de dispersión de happiness_score vs life_exp usando ggplot2.\n\nggplot(world_happiness, aes(life_exp, happiness_score))+\ngeom_point()\n\n\n\n\nAgregamos una línea de tendencia lineal al diagrama de dispersión, configurando se en FALSE.\n\nggplot(world_happiness, aes(life_exp, happiness_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCalcule la correlación entre life_exp y happiness_score\n\ncor(world_happiness$life_exp, world_happiness$happiness_score)\n\n[1] 0.7737615"
  },
  {
    "objectID": "correlacion_de.html#abvertencias-de-correlación",
    "href": "correlacion_de.html#abvertencias-de-correlación",
    "title": "4  Correlación y Diseño Experimental",
    "section": "4.2 Abvertencias de correlación",
    "text": "4.2 Abvertencias de correlación\nSi bien la correlación es una forma útil de cuantificar las relaciones, existen algunas advertencias.\n\n4.2.1 Relaciones no lineales\nConsidere estos datos\n\nClaramente existe una relación entre \\(x\\) e \\(y\\), pero cuando calculamos la correlación, obtenemos 0.18.\nEsto se debe a que la relación entre las dos variables es una relación cuadrática, no una lineal. El coeficiente de correlación mide la fuerza de las relaciones lineales y solo de las relaciones lineales.\nAl igual que cualquier estadística de resumen, la correlación no debe usarse a ciegas y siempre debe visualizar sus datos cuando sea posible.\n\n4.2.1.1 Datos de sueño de mamíferos\nVolvamos a los datos de sueño de los mamiferos que discutimos en el capítulo 1.\n\nlibrary(ggplot2)\ndata(\"msleep\")\n\nhead(msleep)\n\n# A tibble: 6 × 11\n  name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n2 Owl mo… Aotus omni  Prim… &lt;NA&gt;                17         1.8      NA       7  \n3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n6 Three-… Brad… herbi Pilo… &lt;NA&gt;                14.4       2.2       0.767   9.6\n# ℹ 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt;\n\n\n\n\n4.2.1.2 Peso corporal vs tiempo de actividad del día\nAquí hay un diagrama de dispersión del peso corporal de cada mamífero en comparación con el tiempo que pasan dspiertos cada día.\n\nggplot(msleep, aes(bodywt, awake)) + \n  geom_point()\n\n\n\n\nEs claro que la relación entre estas dos variables definitivamente no es lineal.\n\ncor(msleep$bodywt, msleep$awake)\n\n[1] 0.3119801\n\n\nLa correlación entre el peso corporal y el tiempo de vigilancia es solo de 0.3, que es una relación líneal débil.\n\n\n4.2.1.3 Distribución del peso corporal\nSi hechamos un vistazo más de cerca a la distribución del peso corporal, está muy sesgada.\n\nggplot(msleep, aes(bodywt)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHay muchos pesos más bajos y algunos pesos que son mucho más altos que el resto.\n\n\n\n4.2.2 Transformación Log\nCuando los datos estan muy sesgados comoe estos, podemos aplicar una transformación log. Si trazamos el registro del peso corporal frente al tiempo de vigilia, la relación parece mucho más lineal que la que existe entre el peso corporal normal y el tiempo de vigilia.\n\nlibrary(tidyverse)\nmsleep &lt;- msleep %&gt;% \n  mutate(log_bodywt = log(bodywt))\n\nggplot(msleep, aes(log_bodywt, awake)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\ncor(msleep$log_bodywt, msleep$awake)\n\n[1] 0.5687943\n\n\nNote que la correlación entre el registro del pso corporal y el tiempo de vigilia es de aproximadamente 0.57, que es mucho mejor que el 0.3 que teniamos antes.\n\n\n4.2.3 Otras Transformaciones\nAdemás de las transformaciones log(x), hay muchas otras transformaciones que se pueden usar para hacer que una relación sea más lineal, como:\n\nSacar la raíz cuadrada o el recíproco de una variable.\n\nLa elección de la transformación dependerá de los datos y de cuán sesgado estén. Estos se pueden aplicar en diferentes combinaciones a \\(x\\) e \\(y\\), por ejemplo, podría aplicar una transformación logarítmica tanto a \\(x\\) como a \\(y\\), o una transformación de raíz cuadrada a x y una trasnformación recíproca a \\(y\\).\n\n4.2.3.1 ¿Por qué usar una transformación?\nEntonces, ¿por qué usar una transformación? Ciertos métodos estadísticos se basan en variables que tienen una relación líneal, como calcular un coeficiente de correlación. La regresión líneal es otra técnica estadística que requiere que las variables se relacionen de manera líneal, sobre lo que puede aprender todo en este curso.\n\n\n\n4.2.4 Correlación no implica causalidad\nHablemos de otra advertencia importante sobre la correlación de la que quizás haya oído hablar antes: la correlación no implica causalidad. Esto significa que si \\(x\\) e \\(y\\) están correlacionadas, \\(x\\) no necesariamente causa a \\(y\\).\nPor ejemplo, aquí hay un gráfico de dispersión del consumo de margarina per cápita en los EE.UU. Cada año frente a la tasa de divorcio en el estado de Maine.\n\nVea que la correlación entre estas dos variables es 0.99, que es casi perfecta. Sin embargo, esto no significa que consumir más margarina provoque más divorcios. Este tipo de correlación a menudo se denomina correlación espuria.\n\n\n4.2.5 Confusión\n\nUn fenómeno llamado confusión puede dar lugar a correlaciones espurias. Digamos que queremos saber si beber café causa cáncer de pulmón. Al observar los datos, encontramos que el consumo de café y el cáncer de pulmón correlacionados, lo que puede llevarnos a pensar que beber más café le provocará cáncer de pulmón.\nSin embargo, hay una tercera variable oculta en juego, que es fumar.\n\nEn realidad, resulta que el café no provoca cáncer de pulmón y solo está asociado a él, pero parecía causal por la tercera variable, el tabaquismo. Esta tercera variable se denomina factor de confusión o variable oculta. Esto significa que la relación de interés entre el café y el cáncer de pulmón es una correlación espuria.\nOtro ejemplo de esto es la relación entre las ventas minoristas de vacaciones. Si bien puede ser que la gente compré más durante las festividades como una forma de celebración, es difícil saber cuánto del aumento de las ventas se debe a las festividades y cuánto se debe a las ofertas y promociones especiales que a menudo se realizan durante las festividades. \nAquí, las ofertas especiales confunden la relación entre las vacaciones y las ventas.\n\n\n4.2.6 Ejemplo 21\nSi bien el coeficiente de correlación es una forma conveniente de cuantificar la fuerza de una relación entre dos variables, está lejos de ser perfecto. En este ejercicio, explorará una de las advertencias del coeficiente de correlación al examinar la relación entre el PIB per cápita de un país (gdp_per_cap) y el puntaje de felicidad (world_happiness)\n\nPrimero, creemos un diagrama de dispersión que muestre la relación entre gdp_per_capita (en el eje x) y life_exp (en el eje y)\n\n\nworld_happiness &lt;- readRDS(\"world_happiness_sugar.rds\")\n\nggplot(world_happiness, aes(gdp_per_cap, life_exp)) + \n  geom_point()\n\n\n\n\n\nAhora, calculemos la correlación entre gdp_per_cap y life_exp\n\ncor(world_happiness$gdp_per_cap, world_happiness$life_exp)\n\n[1] 0.7235027\n\n\nLa correlación entre el PIB per cápita y la esperanza de vida es de 0.7. ¿Por qué la correlación no es la mejor manera de medir la relación entre las dos variables?\nRespuesta: La correlación solo mide las relaciones lineales.\n\n\n\n4.2.7 Ejemplo 22\nCuando las variables tienen distribuciones sesgadas, a menudo requerimos una transformación para formar una relación lineal con otra variable para que se pueda calcular la correlación. En este ejercicio, usted mismo realizará una transformación.\n\nCree un diagrama de dispersión de happiness_score vs gdp_per_cap y calcular la correlación estre estas variables.\n\nggplot(world_happiness, aes(gdp_per_cap, happiness_score)) + geom_point()\n\n\n\n\n\ncor(world_happiness$gdp_per_cap, world_happiness$happiness_score)\n\n[1] 0.7601853\n\n\nAgregue una nueva columna a world_happiness llamada log_gdp_per_cap que contiene el registro de gdp_per_cap. Cree un diagrama de dispersión de happiness_score vs log_gdp_per_cap y calcule la correlación de estas variables.\n\nworld_happiness &lt;- world_happiness %&gt;%\n  mutate(log_gdp_per_cap = log(gdp_per_cap))\n\n\nggplot(world_happiness, aes(log_gdp_per_cap, happiness_score)) +\n  geom_point()\n\n\n\n\n\ncor(world_happiness$happiness_score, world_happiness$log_gdp_per_cap)\n\n[1] 0.7965484\n\n\n\nTremenda tranformación! Note que la relación entre el PIB per cápita y la felicidad se volvio más lineal al aplicar una transformación logarítmica. Las transformaciones logarítmicas son excelentes para usar en variables con una distribución sesgada como el PIB."
  },
  {
    "objectID": "correlacion_de.html#diseño-de-expermientos",
    "href": "correlacion_de.html#diseño-de-expermientos",
    "title": "4  Correlación y Diseño Experimental",
    "section": "4.3 Diseño de Expermientos",
    "text": "4.3 Diseño de Expermientos\nA menudo, los datos se crean como resultado de un estudio que tiene como objetivo responder a una pregunta específica. Sin embargo, los datos debe analizarse e interpretarse de manera diferente según cómo se generaron los datos y cómo se diseño el estudio.\n\n4.3.1 Vocabulario\nLos experimentos generalmente tienen como objetivo responder una pregunta en la forma:\n\n¿Cuál es el efecto del tratamiento en la respuesta?\n\nEn este contexto, el tratamiento se refiere a la variable explicativa o independiente, y la respuesta se refiere a la respuesta o variable dependiente. Por ejemplo,\n\n¿Cuál es el efecto de un anuncio en la cantidad de productos comprados?\n\nEn este caso, el tratamiento es un anuncio y la respuesta es la cantidad de productos comprados.\n\n\n4.3.2 Experimentos controlados\nEn un experimento controlado, los participantes se asignan aleatoriamente al grupo de tratamiento o al grupo de control, donde el grupo de tratamiento recibe el tratamiento y el grupo de control no.\nEn nuestro ejemplo, el grupo de tratamiento verá un anuncio y el grupo de control no. Aparte de esta diferencia, los grupos deben ser comparables para que podamos determinar si ver un anuncio hace que las personas compren más. si los grupos no son comparables, esto podría generar confusión o sesgo.\nSi la edad promedio de los participantes en el grupo de tratamiento es de 25 años y la edad promedio de los participantes en el grupo de control es de 50, la edad podría ser un posible factor de confusión si es más probable que las personas más jovenes compren más, y lesto hará que el experimento esté sesgado hacia el tratamiento.\n\n\n4.3.3 El patrón oro de los experimentos\nEl estándar de oro, o experimento ideal, eliminará tanto sesgo como sea posible utilizando ciertas herramientas. La primera herramienta para ayudar a eliminar el sesgo en los experimentos controlados es utilizar un ensayo controlado aleatorio. En un ensayo controlado aleatorizado, los participantes se asignan al azar al grupo de tratamiento o de control y su asignación no se basa en nada más que en el azar.\nLa asignación aleatoria como esta ayuda a garantizar que los grupos sean comparables. La segunda forma es usar un placebo, que es algo que se parece al tratamiento, pero no tiene efecto. De esta manera, los participantes no saben si están en el grupo de tratamiento o de control. Esto asegura que el efecto del tratamiento se deba al tratamiento mismo, no a la idea de recibir el tratamiento. Esto es común en los ensayos clínicos que prueban la eficacia de un fármaco.\nEn un experimento doble ciego, la persona que administra el tratamiento o realiza el experimento tampoco sabe si está administrando el tratamiento real o el placebo. Esto protege contra el sesgo en la respuesta, así como el análisis de los resultados. Todas estas diferentes herramientas se reducen al mismo principio: si hay menos oportunidades e que l sesgo se infiltre en su experimento, con mayor confianza podrá concluir si el tratamiento afecta la respuesta.\n\n\n4.3.4 Estudios Observacionales\nEl otro tipo de estudio que discutiremos es el estudio observacional. En un estudio observación, los participantes no se asignan aleatoriamente a los grupos. En cambio, los participantes se asignan a sí mismos, generalmente en función de características preexistentes. Esto es útil para responder preguntas que no conducen a un experimento controlado.\n\nSi desea estudiar el efecto de fumar sobre el cáncer, no puede obligar a las personas a comenzar a fumar.\nDe manera similar, si desea estudiar cómo el comportamiento de compra anterior afecta si alguien comprará un producto, no puede obligar a las personas a tener cierto comportamiento de compra anterior. Debido a que la asignación no es aleatoria, no hay forma de garantizar que los grupos seas comparables en todos los aspectos, por lo que los estudios observacionales no pueden establecer causalidad, solo asociación.\n\nLos efectos del tratamiento pueden confundirse con factores que llevaran a ciertas personas al grupo de control y a otras personas al grupo de tratamiento. Sin embargo, hay formas de controlar los factores de confusión, lo que puede ayudar a fortalecer la confiabilidad de las conclusiones sobre la asociación.\n\n\n4.3.5 Estudios longitudinales vs transversales\nLa última distinción importante a hacer es entre estudios longitudinales y transversales.\n\nEn un estudio longitudinal, se sigue a los mismos participantes durante un período de tiempo para examinar el efecto del tratamiento en la respuesta.\nEn un estudio transversal, los datos se recopilan a partir de una única instantánea en el tiempo. Si queremos investigar el efecto de la edad sobre la estatura, un estudio transversal mediría las estaturas de personas de diferentes edades y las compararía. Sin embargo, los resultados se confundirán con el año de nacimiento y el estilo de vida, ya que es posible que cada generación sea más alta.\n\nEn un estidoo longitudinal, se registraría la estatura de las mismas personas en diferentes momentos de sus vidas, por lo que se elimina la confusión.\nEs importante tener en cuenta que los estudios longitudinales son más caros y tardarán más en realzarse."
  }
]